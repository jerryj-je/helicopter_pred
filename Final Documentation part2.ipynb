{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9e3069-ab7e-4290-b478-b7a3dc7257ba",
   "metadata": {},
   "source": [
    "## <u style=\"text-decoration-thickness: 4px;\">4. Concept and Methodology</span>\n",
    "\n",
    "### **Approach**  \n",
    "The methodology involves processing helicopter flight trajectory data from an HDF5 file, preparing it for machine learning, and applying a LSTM model for path prediction.\n",
    "\n",
    "### **Methodology Breakdown**  \n",
    "1. **Data Loading & Exploration**  \n",
    "   - Access HDF5 file using `h5py`.  \n",
    "   - Identify datasets and convert to Pandas DataFrame.  \n",
    "\n",
    "2. **Data Preprocessing**  \n",
    "   - Extract features and labels.  \n",
    "   - Handle missing values.  \n",
    "\n",
    "3. **Train-Test Split**  \n",
    "   - Partition data using `train_test_split()`.  \n",
    "\n",
    "4. **Model Training**  \n",
    "   - Train `LinearRegression()` model.  \n",
    "\n",
    "5. **Evaluation**  \n",
    "   - Predict and compute Mean Squared Error (MSE).  \n",
    "   - Analyze performance.  \n",
    " \n",
    "- **Interaction of Methods**: The methods follow a sequential pipeline, where data preprocessing ensures quality input for training, and model evaluation verifies performance.  \n",
    "- **Overall Structure**: Data flows from HDF5 storage → preprocessing → model training → evaluation.  \n",
    "- **Boundary Conditions**: The methodology assumes clean data with sufficient features for LSTM. Poor-quality or insufficient data may affect accuracy.  \n",
    "- **Method Selection**: LSTM was chosen for efficiency, while alternative complex models were avoided for initial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c8f1-0382-44ed-8700-e219a7427322",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">HDF5 Data Processing & Linear Regression Model</span>\n",
    "\n",
    "## **1. Import Libraries**  \n",
    "- `h5py`, `numpy`, `pandas`, and `sklearn` for data handling and modeling.\n",
    "\n",
    "## **2. Load & Explore HDF5 File**  \n",
    "- Open the file with `h5py.File()`.  \n",
    "- List datasets using `.visititems()`.  \n",
    "- Convert to a `pandas.DataFrame`.  \n",
    "\n",
    "## **3. Preprocess Data**  \n",
    "- Extract features & labels.  \n",
    "- Handle missing values (if any).  \n",
    "\n",
    "## **4. Train-Test Split**  \n",
    "- Use `train_test_split()` for data partitioning.  \n",
    "\n",
    "## **5. Train & Evaluate Model**  \n",
    "- Train a `LinearRegression()` model.  \n",
    "- Predict and compute **Mean Squared Error (MSE)**.  \n",
    "\n",
    "## **6. Results**  \n",
    "- Print dataset preview.  \n",
    "- Display MSE and analyze performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d1b74a-74a4-4b77-9630-7d41a447d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and groups in the H5 file:\n",
      "float_columns\n"
     ]
    }
   ],
   "source": [
    "##### Import required libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# File name\n",
    "#file_name = \"final_updated_features.h5\"\n",
    "file_name = \"final_updated_features_with_flight_id.h5\"\n",
    "\n",
    "# Step 1: Open the H5 file and explore its structure\n",
    "with h5py.File(file_name, 'r') as h5file:\n",
    "    print(\"Datasets and groups in the H5 file:\")\n",
    "    h5file.visit(print)  # Print all groups and datasets in the filefinal_updated_features.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd00b16-edba-4bb7-b4ce-1ad66b8fe680",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\"> Exploring an HDF5 Dataset</span>\n",
    "\n",
    "## **1. Select a Dataset**  \n",
    "- Identify available datasets in the HDF5 file.  \n",
    "- Replace `'float_columns'` with the actual dataset name.  \n",
    "\n",
    "## **2. Open and Inspect the Dataset**  \n",
    "- Check if the dataset exists in the file.  \n",
    "- Print dataset shape and data type.  \n",
    "\n",
    "## **3. Convert to Pandas DataFrame (If Needed)**  \n",
    "- Load the dataset into a NumPy array.  \n",
    "- Convert to a Pandas DataFrame for further analysis.  \n",
    "\n",
    "## **4. Next Steps**  \n",
    "- Preprocess the dataset (handle missing values, normalization, etc.).  \n",
    "- Visualize data if necessary.  \n",
    "- Use it for machine learning model training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd9404b-03a7-4080-96d2-d6fc28a68730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: float_columns\n",
      "Shape: (25158056, 8)\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Choose a dataset to explore (replace 'your_dataset_name' with the actual dataset name after inspecting the file)\n",
    "dataset_name = 'float_columns'  # Replace this with the actual dataset name\n",
    "\n",
    "with h5py.File(file_name, 'r') as h5file:\n",
    "    if dataset_name in h5file:\n",
    "        data = h5file[dataset_name]\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(f\"Data type: {data.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cc2c1-44e0-447d-91bf-3e9babe87646",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Dataset Exploration Code</span>\n",
    "\n",
    "## **1. Open the HDF5 File**  \n",
    "- The HDF5 file `\"final_updated_features_with_flight_id.h5\"` is opened using `h5py.File()`.  \n",
    "- We check if the dataset `'float_columns'` exists within the file.\n",
    "\n",
    "## **2. Dataset Inspection**  \n",
    "- If the dataset is found, the **shape** and **data type** of the dataset are printed for a quick understanding of its structure.  \n",
    "- Example output would show the dataset's dimensions and the type of data stored (e.g., `float64`).\n",
    "\n",
    "## **3. Preview Data**  \n",
    "- We extract the first **10 rows** of the dataset to inspect the values using slicing (`dataset[:10]`).  \n",
    "- The data is then **converted into a Pandas DataFrame** for better readability and manipulation.\n",
    "\n",
    "## **4. Output Example**  \n",
    "- An example of the output shows the shape, data type, and the first few rows of the dataset, allowing you to validate its content and structure.\n",
    "\n",
    "## **5. Next Steps**  \n",
    "- After exploring the data, you can move to further preprocessing, analysis, and visualization before using the data for machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974786ed-2b37-4130-9a83-c5e91e0037ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'combined_data' has shape: (25158056, 8)\n",
      "[[ 775.  775.   91. ...  320.  180. 7413.]\n",
      " [ 775.  775.   91. ...  320.  180. 7413.]\n",
      " [ 775.  775.   91. ...  320.  180. 7413.]\n",
      " ...\n",
      " [1575. 2375.  123. ... -128.  180. 2479.]\n",
      " [1575. 2375.  123. ... -128.  180. 2479.]\n",
      " [1575. 2375.  123. ... -128.  180. 2479.]]\n"
     ]
    }
   ],
   "source": [
    "##file_name = \"combined_with_flight_id.h5\"\n",
    "# File path\n",
    "file_name = \"final_updated_features_with_flight_id.h5\"\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_name, 'r') as hf:\n",
    "    # Check if the dataset exists\n",
    "    if 'float_columns' in hf:\n",
    "        dataset = hf['float_columns']\n",
    "        \n",
    "        # Print the shape of the dataset\n",
    "        print(f\"Dataset 'combined_data' has shape: {dataset.shape}\")\n",
    "        \n",
    "        # Print the first few rows of the data\n",
    "        print(dataset[:10000])  \n",
    "\n",
    "    else:\n",
    "        print(\"'final_combined_with_flight_id' dataset not found in the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf067f4f-3d68-4805-ab24-262d3e4f8750",
   "metadata": {},
   "source": [
    "#  <u style=\"text-decoration-thickness: 4px;\">Loading Data and Converting to DataFrame</span>\n",
    "\n",
    "## **1. Import Libraries**  \n",
    "- **`h5py`**: To read the HDF5 file.  \n",
    "- **`numpy`**: For array operations.  \n",
    "- **`pandas`**: For handling data in DataFrame format.  \n",
    "- **`sklearn.model_selection`**: For splitting the dataset into training and testing sets.  \n",
    "- **`sklearn.linear_model`**: To apply linear regression.  \n",
    "- **`sklearn.metrics`**: For calculating model performance (e.g., Mean Squared Error).\n",
    "\n",
    "## **2. Load Dataset from HDF5 File**  \n",
    "- The file `\"final_updated_features_with_flight_id.h5\"` is opened using `h5py.File()`.  \n",
    "- The dataset `'float_columns'` is accessed and loaded into memory using `[:]`, which loads the entire dataset.\n",
    "\n",
    "## **3. Convert to Pandas DataFrame**  \n",
    "- The loaded dataset is converted into a **Pandas DataFrame** for easier manipulation and analysis.\n",
    "\n",
    "## **4. Next Steps**  \n",
    "- Once the data is in a DataFrame, you can proceed with preprocessing (e.g., handling missing values) and model training.  \n",
    "- The next step might involve splitting the data into training and testing sets and applying a machine learning model, such as **Linear Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c4cf99-ef08-494f-9b7e-b738ef8b6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "file_name = \"final_updated_features_with_flight_id.h5\"  # Replace with your actual file path\n",
    "with h5py.File(file_name, 'r') as h5_file:\n",
    "    dataset = h5_file['float_columns'][:]  # Load the entire dataset into memory\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1573a1fc-2d6b-4360-a378-d72f335755c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     2          3         4      5      6       7\n",
      "0  775.0  775.0  91.0  53.473299  9.945374  320.0  180.0  7413.0\n",
      "1  775.0  775.0  91.0  53.473299  9.945374  320.0  180.0  7413.0\n",
      "2  775.0  775.0  91.0  53.473299  9.945374  320.0  180.0  7413.0\n",
      "3  775.0  775.0  91.0  53.473299  9.945374  320.0  180.0  7413.0\n",
      "4  775.0  775.0  91.0  53.473299  9.945374  320.0  180.0  7413.0\n",
      "               0       1      2          3          4    5      6        7\n",
      "25158051  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0  21105.0\n",
      "25158052  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0  21105.0\n",
      "25158053  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0  21105.0\n",
      "25158054  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0  21105.0\n",
      "25158055  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0  21105.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5bfd2-22ff-4a5c-af1b-2905afa98c10",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Assigning Custom Column Names</span>\n",
    "\n",
    "## **1. Define Custom Column Names**  \n",
    "- A list of custom column names is provided for the dataset:\n",
    "  ```python\n",
    "  column_names = [ 'altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude', 'vertical_rate', 'compute_gs', 'flight_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fb7b5f-54a6-45a9-9077-794ea3a29997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude',\n",
      "       'vertical_rate', 'compute_gs', 'flight_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assign custom column names based on the list you provided\n",
    "column_names = [ 'altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude', 'vertical_rate', 'compute_track','flight_id']\n",
    "\n",
    "# Assign these column names to the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "# Check if the column names are assigned correctly\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f687fae-be9f-4aae-adcc-a11d74d6c571",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Converting Latitude and Longitude to Cartesian Coordinates</span>\n",
    "\n",
    "## **1. Import Libraries**  \n",
    "- **`numpy`**: For mathematical operations (like converting degrees to radians).  \n",
    "- **`pandas`**: For DataFrame manipulation.\n",
    "\n",
    "## **2. Convert Latitude and Longitude to Radians**  \n",
    "- The columns `'latitude'` and `'longitude'` are converted from degrees to radians:\n",
    "  ```python\n",
    "  df['lat_rad'] = np.radians(df['latitude'])  # Convert latitude to radians\n",
    "  df['lon_rad'] = np.radians(df['longitude'])  # Convert longitude to radians\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad32f609-8ff8-4280-bdfe-6e298bd22fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitude  geoaltitude  groundspeed  vertical_rate  compute_gs  flight_id  \\\n",
      "0     775.0        775.0         91.0          320.0       180.0     7413.0   \n",
      "1     775.0        775.0         91.0          320.0       180.0     7413.0   \n",
      "2     775.0        775.0         91.0          320.0       180.0     7413.0   \n",
      "3     775.0        775.0         91.0          320.0       180.0     7413.0   \n",
      "4     775.0        775.0         91.0          320.0       180.0     7413.0   \n",
      "\n",
      "          x         y        z  \n",
      "0  0.586253  0.102796  0.80358  \n",
      "1  0.586253  0.102796  0.80358  \n",
      "2  0.586253  0.102796  0.80358  \n",
      "3  0.586253  0.102796  0.80358  \n",
      "4  0.586253  0.102796  0.80358  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'latitude' and 'longitude' columns in degrees\n",
    "df['lat_rad'] = np.radians(df['latitude'])  # Convert latitude to radians\n",
    "df['lon_rad'] = np.radians(df['longitude'])  # Convert longitude to radians\n",
    "\n",
    "# Compute x, y, z coordinates\n",
    "df['x'] = np.cos(df['lat_rad']) * np.cos(df['lon_rad'])\n",
    "df['y'] = np.cos(df['lat_rad']) * np.sin(df['lon_rad'])\n",
    "df['z'] = np.sin(df['lat_rad'])\n",
    "\n",
    "# Drop original latitude, longitude, and intermediate radian columns\n",
    "df.drop(columns=['latitude', 'longitude', 'lat_rad', 'lon_rad'], inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ac0e6a-ea64-4718-9f21-18d442937ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"flight_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd99a3b-7451-4bdb-9233-46279041454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"flight_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e81610-b65b-463a-8189-3ce06cd0bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after sorting and removing the flight_id column:\n",
      "       altitude  geoaltitude  groundspeed  vertical_rate  compute_gs  \\\n",
      "11797    1400.0       1375.0        132.0            0.0       180.0   \n",
      "88921     450.0        425.0        116.0           64.0       180.0   \n",
      "88922     475.0        425.0        115.0           64.0       180.0   \n",
      "88923     475.0        450.0        115.0           64.0       180.0   \n",
      "88924     475.0        450.0        115.0           64.0       180.0   \n",
      "\n",
      "              x         y         z  \n",
      "11797  0.586926  0.104000  0.802933  \n",
      "88921  0.586205  0.103102  0.803575  \n",
      "88922  0.586226  0.103064  0.803565  \n",
      "88923  0.586226  0.103064  0.803565  \n",
      "88924  0.586226  0.103064  0.803565  \n"
     ]
    }
   ],
   "source": [
    "print(\"Data after sorting and removing the flight_id column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89c8c5-5b3e-4d84-886d-31acd92a362f",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Splitting Data into Input (X) and Output (y)</span>\n",
    "\n",
    "## **1. Importing Libraries**  \n",
    "- **`numpy`**: For array manipulation and handling data.\n",
    "\n",
    "## **2. Splitting the DataFrame**  \n",
    "- The dataset is split into two parts:  \n",
    "  - **Input (X)**: This will contain the features used to train the model.  \n",
    "  - **Output (y)**: This will contain the target variables that the model will predict.\n",
    "\n",
    "### **a. Selecting Input (X)**  \n",
    "- The input variable **X** includes all columns from the DataFrame:  \n",
    "  ```python\n",
    "  X = df.values  # Use all columns as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd94b342-08ae-4ab0-a93b-fff8b1455ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25158056, 8)\n",
      "y shape: (25158056, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X = df.values  # Use all columns as input\n",
    "y = df.iloc[:, [0, 5, 6, 7]].values  # Select columns 1, 4, and 5 (0-based index)\n",
    "\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "#X = df.iloc[:, 3:].values  # Use all columns except the first 3 as input\n",
    "#y = df.iloc[:, :3].values  # Use the first 3 columns as output\n",
    "\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c0529-b727-44c1-87d5-e0e9f420f863",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\"> Splitting the Data into Train and Test Sets</span>\n",
    "\n",
    "## **1. Define Train-Test Split Ratio**  \n",
    "- The **train ratio** is set to 80%, meaning 80% of the data will be used for training and 20% for testing:  \n",
    "  ```python\n",
    "  train_ratio = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2858e90-b0ec-4f5d-bbdb-b07c9bd0a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20126444, 8)\n",
      "y_train shape: (20126444, 4)\n",
      "X_test shape: (5031612, 8)\n",
      "y_test shape: (5031612, 4)\n"
     ]
    }
   ],
   "source": [
    "## splitting into train and test\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(df) * train_ratio)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7841b62-c991-47e3-a503-cea95acfb411",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Time-Series Data Preparation for Forecasting</span>\n",
    "\n",
    "This code snippet is designed to prepare training and test datasets for a time-series forecasting model, specifically aiming to predict values 60 timesteps ahead.\n",
    "\n",
    "#### 1. **Parameters**:\n",
    "   - `n_target_steps = 60`: Specifies the number of timesteps into the future that the model is expected to predict.\n",
    "\n",
    "#### 2. **Training Data Preparation**:\n",
    "   - `X_train_new` and `y_train_new` are initialized as empty lists to store the new input-output pairs for training.\n",
    "   - A loop iterates through the original training data (`X_train` and `y_train`):\n",
    "     - Each input sample consists of the current timestep (`X_train[i, :]`).\n",
    "     - The output is the value 60 timesteps ahead (`y_train[i + n_target_steps, :]`).\n",
    "   - After the loop, the lists `X_train_new` and `y_train_new` are converted into NumPy arrays (`X_train` and `y_train`).\n",
    "\n",
    "#### 3. **Test Data Preparation**:\n",
    "   - Similarly, `X_test_new` and `y_test_new` are initialized for test data processing.\n",
    "   - A loop processes the `X_test` and `y_test` in the same way, generating input-output pairs where the output is 60 timesteps ahead of the input.\n",
    "\n",
    "#### 4. **Final Shape Check**:\n",
    "   - The shapes of the final training and test datasets (`X_train`, `y_train`, `X_test`, and `y_test`) are printed. The expected shapes are:\n",
    "     - `X_train` and `X_test`: `(samples, features)`\n",
    "     - `y_train` and `y_test`: `(samples, target_features)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de318b1e-4082-4d50-bcd9-b3e5da5f3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20126384, 8)\n",
      "y_train shape: (20126384, 4)\n",
      "X_test shape: (5031552, 8)\n",
      "y_test shape: (5031552, 4)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_target_steps = 60  # Predict 60 timesteps into the future\n",
    "\n",
    "# Initialize lists for inputs and outputs\n",
    "X_train_new = []\n",
    "y_train_new = []\n",
    "\n",
    "# Create training data\n",
    "for i in range(len(X_train) - n_target_steps):\n",
    "    # Current timestep as input\n",
    "    X_train_new.append(X_train[i, :])  # Take the current timestep (single row)\n",
    "    # Value 60 timesteps into the future as output\n",
    "    y_train_new.append(y_train[i + n_target_steps, :])  # Predict 60 timesteps ahead\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_train_new)\n",
    "y_train = np.array(y_train_new)\n",
    "\n",
    "# Do the same for the test set\n",
    "X_test_new = []\n",
    "y_test_new = []\n",
    "\n",
    "for i in range(len(X_test) - n_target_steps):\n",
    "    X_test_new.append(X_test[i, :])\n",
    "    y_test_new.append(y_test[i + n_target_steps, :])\n",
    "\n",
    "X_test = np.array(X_test_new)\n",
    "y_test= np.array(y_test_new)\n",
    "\n",
    "# Check shapes\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, features)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (samples, target_features)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e832b9-6f79-4c51-8b65-e0fd2425aa31",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Feature Scaling for Time-Series Data</span>\n",
    "\n",
    "This code snippet applies feature scaling using `MinMaxScaler` from `sklearn` to transform the input features and target values of a time-series dataset. The goal is to normalize the data for improved model performance.\n",
    "\n",
    "#### 1. **Scaler Initialization**:\n",
    "   - Four `MinMaxScaler` objects are created:\n",
    "     - `scaler_X_altitude`: Scales the altitude feature in the input data (`X`) to a range of [0, 1].\n",
    "     - `scaler_X_other`: Scales other features in `X` to a range of [-1, 1].\n",
    "     - `scaler_y_altitude`: Scales the output altitude (`y`) to a range of [0, 1].\n",
    "     - `scaler_y_other`: Scales the other output columns (`y`) to a range of [-1, 1].\n",
    "\n",
    "#### 2. **Separation of Altitude and Other Features**:\n",
    "   - The altitude column is isolated from both the input features (`X_train` and `X_test`) and target values (`y_train` and `y_test`), using `altitude_index = 0` as the reference for the altitude column.\n",
    "   - The altitude values are reshaped to be a 2D array, while the remaining features are stored separately in `X_train_other` and `X_test_other` for the input, and `y_train_other` and `y_test_other` for the target.\n",
    "\n",
    "#### 3. **Scaling**:\n",
    "   - The altitude features are scaled between 0 and 1 using `scaler_X_altitude` and `scaler_y_altitude`.\n",
    "   - The remaining features and target values are scaled between -1 and 1 using `scaler_X_other` and `scaler_y_other`.\n",
    "\n",
    "#### 4. **Recombining Scaled Features**:\n",
    "   - After scaling, the altitude features (both input and output) are combined with the other features (input and output) to form the final scaled input (`X_train`, `X_test`) and target (`y_train`, `y_test`) datasets using `np.hstack()`.\n",
    "\n",
    "#### 5. **Shape Check**:\n",
    "   - The shapes of the scaled datasets (`X_train`, `y_train`, `X_test`, `y_test`) are printed to ensure the dimensions remain consistent after scaling.\n",
    "   \n",
    "#### 6. **Optional Output**:\n",
    "   - The first few rows of the scaled training data are printed for inspection, allowing verification of the transformed values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33401afc-cd1d-420a-874f-f21b86b5b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (20126384, 8)\n",
      "y_train_scaled shape: (20126384, 4)\n",
      "X_test_scaled shape: (5031552, 8)\n",
      "y_test_scaled shape: (5031552, 4)\n",
      "[[ 2.03125000e-02 -9.62580394e-01 -8.73015873e-01  4.23728814e-02\n",
      "  -1.04805054e-13  5.82042289e-01  2.44283662e-01  5.55616578e-01]\n",
      " [ 1.28906250e-02 -9.77392321e-01 -8.88407888e-01  4.51977401e-02\n",
      "  -1.04805054e-13  5.80401163e-01  2.43010181e-01  5.61400906e-01]\n",
      " [ 1.30859375e-02 -9.77392321e-01 -8.89369889e-01  4.51977401e-02\n",
      "  -1.04805054e-13  5.80449006e-01  2.42955740e-01  5.61307183e-01]\n",
      " [ 1.30859375e-02 -9.77002534e-01 -8.89369889e-01  4.51977401e-02\n",
      "  -1.04805054e-13  5.80449006e-01  2.42955740e-01  5.61307183e-01]\n",
      " [ 1.30859375e-02 -9.77002534e-01 -8.89369889e-01  4.51977401e-02\n",
      "  -1.04805054e-13  5.80449006e-01  2.42955740e-01  5.61307183e-01]]\n",
      "[[0.01230469 0.58061868 0.24267503 0.56104589]\n",
      " [0.01230469 0.58053663 0.24266607 0.56129005]\n",
      " [0.01230469 0.58053663 0.24266607 0.56129005]\n",
      " [0.01230469 0.58065747 0.24253726 0.56104589]\n",
      " [0.0125     0.58065424 0.24258361 0.56101758]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create MinMaxScaler objects\n",
    "scaler_X_altitude = MinMaxScaler(feature_range=(0, 1))  # For altitude to scale between 0 and 1\n",
    "scaler_X_other = MinMaxScaler(feature_range=(-1, 1))  # For other features to scale between -1 and 1\n",
    "scaler_y_altitude = MinMaxScaler(feature_range=(0, 1))  # For output altitude to scale between 0 and 1\n",
    "scaler_y_other = MinMaxScaler(feature_range=(-1, 1))  # For other output columns to scale between -1 and 1\n",
    "\n",
    "# Identify the altitude column index\n",
    "altitude_index = 0  # Change this based on your dataset\n",
    "\n",
    "# Separate the altitude column from X (features)\n",
    "altitude_train_X = X_train[:, altitude_index].reshape(-1, 1)\n",
    "X_train_other = X_train[:, [i for i in range(X_train.shape[1]) if i != altitude_index]]\n",
    "\n",
    "altitude_test_X = X_test[:, altitude_index].reshape(-1, 1)\n",
    "X_test_other = X_test[:, [i for i in range(X_test.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Separate the altitude column from y (target)\n",
    "altitude_train_y = y_train[:, altitude_index].reshape(-1, 1)\n",
    "y_train_other = y_train[:, [i for i in range(y_train.shape[1]) if i != altitude_index]]\n",
    "\n",
    "altitude_test_y = y_test[:, altitude_index].reshape(-1, 1)\n",
    "y_test_other = y_test[:, [i for i in range(y_test.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Scale the altitude feature in X to [0, 1]\n",
    "altitude_train_X_scaled = scaler_X_altitude.fit_transform(altitude_train_X)\n",
    "altitude_test_X_scaled = scaler_X_altitude.transform(altitude_test_X)\n",
    "\n",
    "# Scale the other features in X to [-1, 1]\n",
    "X_train_other_scaled = scaler_X_other.fit_transform(X_train_other)\n",
    "X_test_other_scaled = scaler_X_other.transform(X_test_other)\n",
    "\n",
    "# Scale the altitude in y to [0, 1]\n",
    "altitude_train_y_scaled = scaler_y_altitude.fit_transform(altitude_train_y)\n",
    "altitude_test_y_scaled = scaler_y_altitude.transform(altitude_test_y)\n",
    "\n",
    "# Scale the other columns in y to [-1, 1]\n",
    "y_train_other_scaled = scaler_y_other.fit_transform(y_train_other)\n",
    "y_test_other_scaled = scaler_y_other.transform(y_test_other)\n",
    "\n",
    "# Combine the scaled altitude with the scaled other features in X\n",
    "X_train = np.hstack([altitude_train_X_scaled, X_train_other_scaled])\n",
    "X_test = np.hstack([altitude_test_X_scaled, X_test_other_scaled])\n",
    "\n",
    "# Combine the scaled altitude with the scaled other columns in y\n",
    "y_train= np.hstack([altitude_train_y_scaled, y_train_other_scaled])\n",
    "y_test = np.hstack([altitude_test_y_scaled, y_test_other_scaled])\n",
    "\n",
    "# Check the shapes after scaling (should remain the same)\n",
    "print(\"X_train_scaled shape:\", X_train.shape)\n",
    "print(\"y_train_scaled shape:\", y_train.shape)\n",
    "print(\"X_test_scaled shape:\", X_test.shape)\n",
    "print(\"y_test_scaled shape:\", y_test.shape)\n",
    "\n",
    "# Optional: Print the first few rows of the scaled training data\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceaa6f-3cc6-4fb8-86c4-791b4ed82456",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Overview of Model Serialization for Scalers</span>\n",
    "\n",
    "This code snippet demonstrates how to save the trained `MinMaxScaler` objects to disk using `joblib` for future use.\n",
    "\n",
    "#### 1. **Saving Scalers**:\n",
    "   - The following scalers are serialized (saved) as `.pkl` files using `joblib.dump()`:\n",
    "     - `scaler_X_altitude`: The scaler for scaling the altitude feature in the input data (`X`).\n",
    "     - `scaler_X_other`: The scaler for scaling other features in the input data (`X`).\n",
    "     - `scaler_y_altitude`: The scaler for scaling the altitude feature in the target data (`y`).\n",
    "     - `scaler_y_other`: The scaler for scaling other target columns in the target data (`y`).\n",
    "\n",
    "   These serialized scaler objects can later be reloaded into another script or model to ensure that the same scaling transformations are applied consistently during inference or testi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a9c526-9837-415c-9e25-69b18253be62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_y_other.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler_X_altitude, 'scaler_X_altitude.pkl')\n",
    "joblib.dump(scaler_X_other, 'scaler_X_other.pkl')\n",
    "joblib.dump(scaler_y_altitude, 'scaler_y_altitude.pkl')\n",
    "joblib.dump(scaler_y_other, 'scaler_y_other.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bc55b-4363-4dc4-9a05-3ade0637b639",
   "metadata": {},
   "source": [
    "## <u style=\"text-decoration-thickness: 4px;\">Dimension Expansion for LSTM Input</span>\n",
    "\n",
    "This code snippet adjusts the dimensions of the input data to make it compatible with the requirements of a Long Short-Term Memory (LSTM) network.\n",
    "\n",
    "#### 1. **Expanding Dimensions**:\n",
    "   - The `np.expand_dims()` function is used to add an additional dimension to the input datasets (`X_train` and `X_test`), converting them into 3D arrays:\n",
    "     - The new shape becomes `(samples, time_steps, features)`, where:\n",
    "       - `samples` represents the number of data points.\n",
    "       - `time_steps` is set to 1 since we are processing one timestep at a time (i.e., no sequence length).\n",
    "       - `features` refers to the number of features in the dataset.\n",
    "   - Specifically, the expansion happens along the second axis (axis=1), which is the time-step dimension.\n",
    "\n",
    "#### 2. **Shape of Data**:\n",
    "   - After the operation, the shape of `X_train` and `X_test` changes from `(samples, features)` to `(samples, 1, features)`. This ensures the data fits the expected input format for LSTM models, which require 3D data.\n",
    "\n",
    "#### 3. **Print Shape for Verification**:\n",
    "   - The shapes of `X_train` and `X_test` after the expansion are printed to verify the transformation. The expected output is:\n",
    "     - `X_train shape after expansion: (samples, 1, features)`\n",
    "     - `X_test shape after expansion: (samples, 1, features)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c761340-027c-4c37-af73-616f667ae8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after expansion: (20126384, 1, 8)\n",
      "X_test shape after expansion: (5031552, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "# Expand dimensions for LSTM input (samples, time_steps, features)\n",
    "X_train = np.expand_dims(X_train, axis=1)  # Shape becomes (samples, 1, features)\n",
    "X_test = np.expand_dims(X_test, axis=1)    \n",
    "\n",
    "print(\"X_train shape after expansion:\", X_train.shape)\n",
    "print(\"X_test shape after expansion:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366649f-9df0-42b3-9303-54b058620cdd",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">LSTM Model for Time-Series Forecasting</span>\n",
    "\n",
    "This code snippet defines, compiles, trains, and evaluates a Long Short-Term Memory (LSTM) model for a time-series forecasting task.\n",
    "\n",
    "#### 1. **Model Definition**:\n",
    "   - A Sequential model is created using Keras:\n",
    "     - **LSTM Layer**: \n",
    "       - The LSTM layer is added with 7 units (neurons) and uses the ReLU activation function.\n",
    "       - The `input_shape` is defined as `(X_train.shape[1], X_train.shape[2])`, which corresponds to the time steps (1) and the number of features in the input data.\n",
    "     - **Dense Layer**: \n",
    "       - A Dense output layer is added with `y_train.shape[1]` units, which represents the number of target columns (i.e., 3 target variables).\n",
    "  \n",
    "#### 2. **Model Compilation**:\n",
    "   - The model is compiled using the Adam optimizer (`'adam'`) and Mean Squared Error (`'mse'`) as the loss function. The Adam optimizer is commonly used for training deep learning models due to its efficiency and adaptability.\n",
    "\n",
    "#### 3. **Model Training**:\n",
    "   - The model is trained using the `fit()` function:\n",
    "     - Training is performed for 20 epochs with a batch size of 64.\n",
    "     - The training progress is printed (`verbose=1`), and validation data (`X_test` and `y_test`) is provided to evaluate the model on unseen data during training.\n",
    "\n",
    "#### 4. **Model Evaluation**:\n",
    "   - After training, the model is evaluated using the `evaluate()` function on the test set (`X_test`, `y_test`), and the test loss is printed.\n",
    "\n",
    "   - The expected output will display the test loss, which indicates how well the model performs on the unseen test data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0cef25c-aca1-458c-b732-69300e3bf9b8",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(y_train.shape[1])  # Output layer: Predict the 3 target columns\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c7a24-3681-46ad-97f2-a93b740f7267",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Enhanced LSTM Model with Regularization and Early Stopping</span>\n",
    "\n",
    "This code snippet enhances the previous LSTM model by adding more layers, applying dropout for regularization, and implementing early stopping during training.\n",
    "\n",
    "#### 1. **Model Definition**:\n",
    "   - **LSTM Layers**:\n",
    "     - Three LSTM layers with 50 units each are added to the model. \n",
    "     - Each LSTM layer uses the **tanh** activation function instead of ReLU to capture more complex relationships in the data.\n",
    "     - The first two LSTM layers return sequences (`return_sequences=True`) to pass a sequence of outputs to the next layer, while the final LSTM layer does not return sequences, producing the final output for prediction.\n",
    "   - **Dropout Layers**:\n",
    "     - Dropout layers with a rate of 0.2 are added after each LSTM layer. Dropout helps prevent overfitting by randomly setting a fraction of the input units to zero during training.\n",
    "   - **Dense Layer**:\n",
    "     - A Dense output layer is added with a number of units equal to `y_train.shape[1]` (i.e., the number of target features).\n",
    "\n",
    "#### 2. **Model Compilation**:\n",
    "   - The model is compiled with the Adam optimizer (`'adam'`) and Mean Squared Error (`'mse'`) as the loss function, which is commonly used for regression problems like time-series forecasting.\n",
    "\n",
    "#### 3. **Early Stopping**:\n",
    "   - The **EarlyStopping** callback is defined:\n",
    "     - It monitors the validation loss (`'val_loss'`) to detect if the model’s performance is not improving.\n",
    "     - The training process will stop if the validation loss does not improve for 3 consecutive epochs (`patience=3`).\n",
    "     - The best model weights are restored after training stops (`restore_best_weights=True`), ensuring the best model is retained.\n",
    "\n",
    "#### 4. **Model Training**:\n",
    "   - The model is trained with the `fit()` function:\n",
    "     - Training runs for up to 20 epochs with a batch size of 64.\n",
    "     - The `validation_data` is passed to monitor the performance on the test set during training.\n",
    "     - The **EarlyStopping** callback is used to prevent overfitting and stop training once the model performance plateaus.\n",
    "\n",
    "#### 5. **Model Evaluation**:\n",
    "   - After training, the model is evaluated on the test set (`X_test`, `y_test`) using the `evaluate()` function.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b6acfa-e4c7-4156-89a8-b3fb5bda3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 13:40:15.753977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 6.3799e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 2ms/step - loss: 8.5705e-04 - val_loss: 6.5319e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 2ms/step - loss: 8.5081e-04 - val_loss: 6.5730e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 2ms/step - loss: 8.4274e-04 - val_loss: 6.5131e-04\n",
      "\u001b[1m157236/157236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 775us/step - loss: 6.8818e-04\n",
      "Test Loss: 0.0006379964761435986\n"
     ]
    }
   ],
   "source": [
    "#ading early stopping and chnaging relu to tanh, adding dropout layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))  # Add dropout for regularization\n",
    "\n",
    "model.add(LSTM(50, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(50, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(y_train.shape[1])) \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitors the validation loss\n",
    "    patience=3,           # Number of epochs to wait for improvement before stopping\n",
    "    restore_best_weights=True  # Restores the best model weights after stopping\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=[early_stopping]  # Pass EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "##0.0090 as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bf444-2b1f-4321-9f7f-b9c672018793",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Prediction and Evaluation on Test Set</span>\n",
    "\n",
    "This code snippet generates predictions for the test set and compares the predicted values to the actual target values.\n",
    "\n",
    "#### 1. **Generating Predictions**:\n",
    "   - The model's `predict()` function is used to generate predictions on the test set (`X_test`). These predictions are stored in `y_pred`.\n",
    "\n",
    "#### 2. **Displaying Predicted vs Actual Values**:\n",
    "   - The code prints a comparison of the first few predicted values (`y_pred`) versus the actual values (`y_test`) from the test set. \n",
    "   - A loop is used to display 5 examples (specified by `num_examples = 5`):\n",
    "     - For each example, the predicted and actual values are printed.\n",
    "     - A separator line (`\"-\" * 30`) is used for clarity between examples.\n",
    "\n",
    "This step allows for visual inspection of the model's performance by comparing the predicted outputs with the ground truth, providing insight into how well the model generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8680bd6b-abb4-43a4-892a-33da74888323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157236/157236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 834us/step\n",
      "Predicted vs Actual Values:\n",
      "Example 1:\n",
      "Predicted: [0.02834139 0.74597764 0.25983354 0.02050689]\n",
      "Actual: [0.02832031 0.74902057 0.25640524 0.01169354]\n",
      "------------------------------\n",
      "Example 2:\n",
      "Predicted: [0.02835777 0.74599934 0.25999257 0.02025291]\n",
      "Actual: [0.02832031 0.74902057 0.25640524 0.01169354]\n",
      "------------------------------\n",
      "Example 3:\n",
      "Predicted: [0.02835828 0.7460049  0.25998527 0.02024028]\n",
      "Actual: [0.02851563 0.74904069 0.2563979  0.01162982]\n",
      "------------------------------\n",
      "Example 4:\n",
      "Predicted: [0.02834491 0.7459714  0.25986397 0.02048442]\n",
      "Actual: [0.02832031 0.74907141 0.25638748 0.01153179]\n",
      "------------------------------\n",
      "Example 5:\n",
      "Predicted: [0.02835853 0.74600756 0.25998178 0.0202342 ]\n",
      "Actual: [0.02832031 0.74907141 0.25638748 0.01153179]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print a few examples of predictions and actual values\n",
    "num_examples = 5  # Number of examples to display\n",
    "print(\"Predicted vs Actual Values:\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Predicted: {y_pred[i]}\")\n",
    "    print(f\"Actual: {y_test[i]}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4e92b8-5fe1-4f20-933f-079df91a7a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model,'model.pkl')\n",
    "#model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b7d51fd-a1d5-4e04-9540-2bdac64d479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 14:35:58.156363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4357f-6c34-4966-b8fa-e3b95615c5c1",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Data Loading and Merging from Multiple CSV Files</span>\n",
    "\n",
    "This code snippet demonstrates how to load multiple CSV files, process them, and combine them into a single DataFrame. The files represent test data for multiple years and months, and the goal is to merge them for further analysis.\n",
    "\n",
    "#### 1. **Directory Setup**:\n",
    "   - The directory path `/home/jovyan/data/data_test` is specified, where the CSV files are stored.\n",
    "\n",
    "#### 2. **Iterating Over Files**:\n",
    "   - A loop iterates over years from 2017 to 2021 and months from 1 to 12.\n",
    "   - For each year-month combination, the corresponding file name is generated (e.g., `test_set_1_2017.csv` for January 2017).\n",
    "   - The full file path is constructed using `os.path.join()`.\n",
    "\n",
    "#### 3. **File Loading**:\n",
    "   - Inside a `try-except` block, the code attempts to load each CSV file into a Pandas DataFrame using `pd.read_csv()`.\n",
    "   - The `timestamp` column is converted into a `datetime` format to ensure consistent date handling.\n",
    "\n",
    "#### 4. **Handling Missing Files**:\n",
    "   - If a file is not found (`FileNotFoundError`), it is skipped, and the loop continues to the next file.\n",
    "\n",
    "#### 5. **Merging Data**:\n",
    "   - All successfully loaded DataFrames are appended to a list (`data_frames`).\n",
    "   - After iterating through all the files, the DataFrames are concatenated into a single DataFrame using `pd.concat()`, with `ignore_index=True` to reset the index.\n",
    "\n",
    "#### 6. **Summary**:\n",
    "   - The code prints the number of CSV files that were successfully loaded and merged into the final DataFrame (`merged_data`), providing insight into the data loading process.\n",
    "\n",
    "This process allows the user to efficiently load, process, and merge large amounts of time-series data spread across multiple files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6668c36-7693-480c-8589-a78d8c3c89d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files successfully loaded: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_frames = []  # List to hold DataFrames\n",
    "data_dir = \"/home/jovyan/data/data_test\"  # Data directory\n",
    "\n",
    "# Counter for successfully loaded files\n",
    "loaded_files_count = 0\n",
    "\n",
    "# Looping through all years and months\n",
    "for year in range(2017, 2022):  # From 2017 to 2021\n",
    "    for month in range(1, 13):  # Months from 1 to 12\n",
    "        # Construct the full file path\n",
    "        file_name = f\"test_set_{month}_{year}.csv\"  # The file name\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load the data into a DataFrame\n",
    "            temp_data = pd.read_csv(file_path, sep=',')\n",
    "            \n",
    "            # Convert the timestamp column to datetime format\n",
    "            temp_data['timestamp'] = pd.to_datetime(temp_data['timestamp'])\n",
    "            \n",
    "            # Add the DataFrame to the list\n",
    "            data_frames.append(temp_data)\n",
    "            \n",
    "            # Increment the counter for successfully loaded files\n",
    "            loaded_files_count += 1\n",
    "        except FileNotFoundError:\n",
    "            # Skip the file\n",
    "            pass\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Print the count of successfully loaded files\n",
    "print(f\"Number of CSV files successfully loaded: {loaded_files_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34989e47-9a33-454c-8061-285d3f453c1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57df7019-4b9b-4742-b72d-ed42bd8a4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  alert  \\\n",
      "0           108       79969       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "1           109       79970       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "2           110       79971       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "3           111       79972       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "4           112       79973       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "\n",
      "   altitude  count  geoaltitude  ...  squawk           timestamp      track  \\\n",
      "0    1275.0      2       1550.0  ...     NaN 2017-08-01 07:18:29  315.00000   \n",
      "1    1325.0      2       1600.0  ...     NaN 2017-08-01 07:18:30  293.19858   \n",
      "2    1325.0      2       1625.0  ...     NaN 2017-08-01 07:18:31  294.44397   \n",
      "3    1300.0      2       1625.0  ...     NaN 2017-08-01 07:18:32  294.44397   \n",
      "4    1300.0      2       1600.0  ...     NaN 2017-08-01 07:18:33  291.25052   \n",
      "\n",
      "   vertical_rate   flight_id   cumdist  compute_gs  compute_track month  \\\n",
      "0          576.0  3E0F76_969  0.000000   10.327583      270.00003     8   \n",
      "1          320.0  3E0F76_969  0.002869   10.327583      270.00003     8   \n",
      "2           64.0  3E0F76_969  0.002869    0.000000      180.00000     8   \n",
      "3           64.0  3E0F76_969  0.011918   32.575943      287.99356     8   \n",
      "4         -192.0  3E0F76_969  0.011918    0.000000      180.00000     8   \n",
      "\n",
      "   time_diff  \n",
      "0        NaN  \n",
      "1        1.0  \n",
      "2        1.0  \n",
      "3        1.0  \n",
      "4        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "         Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  \\\n",
      "2177001       1452126      273576       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177002       1452127      273577       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177003       1452128      273578       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177004       1452129      273579       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177005       1452130      273580       D-HZSD  3E167B      BMI    CHX29   \n",
      "\n",
      "         alert  altitude  count  geoaltitude  ...  squawk           timestamp  \\\n",
      "2177001  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:02   \n",
      "2177002  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:03   \n",
      "2177003  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:04   \n",
      "2177004  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:05   \n",
      "2177005  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:06   \n",
      "\n",
      "            track  vertical_rate   flight_id    cumdist  compute_gs  \\\n",
      "2177001  1.974934           64.0  CHX29_1765  11.544259   70.057920   \n",
      "2177002  1.992094         -128.0  CHX29_1765  11.544259    0.000000   \n",
      "2177003  1.992094            0.0  CHX29_1765  11.618588  267.582180   \n",
      "2177004  2.009554         -192.0  CHX29_1765  11.651598  118.840385   \n",
      "2177005  2.511363         -192.0  CHX29_1765  11.685999  123.841286   \n",
      "\n",
      "         compute_track month  time_diff  \n",
      "2177001       8.303264     7        1.0  \n",
      "2177002     180.000000     7        1.0  \n",
      "2177003       2.166752     7        1.0  \n",
      "2177004     360.000000     7        1.0  \n",
      "2177005       3.995433     7        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.head())\n",
    "print(merged_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce07115-f5c6-4581-bb67-0c4e47496fa4",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Timestamp Processing and Flight ID Modification</span>\n",
    "\n",
    "This code snippet processes the timestamp column and modifies the `flight_id` column in the merged dataset.\n",
    "\n",
    "#### 1. **Convert Timestamp to Datetime**:\n",
    "   - The code ensures that the `timestamp` column in the `merged_data` DataFrame is in `datetime` format using `pd.to_datetime()`. If it’s already in the correct format, this step has no effect.\n",
    "\n",
    "#### 2. **Extract Year and Month**:\n",
    "   - The `year_month` variable is created by extracting the year and month from the `timestamp` column. The `dt.strftime('%Y_%m')` method formats the year and month into the format `YYYY_MM` (e.g., `2021_01` for January 2021).\n",
    "\n",
    "#### 3. **Modify the `flight_id` Column**:\n",
    "   - The `flight_id` column is modified by appending the extracted `year_month` to each existing `flight_id`. This creates a new unique identifier for each flight that combines the `flight_id` with the corresponding year and month of the data (e.g., `flight123_2021_01` for January 2021).\n",
    "\n",
    "#### 4. **Display the Updated DataFrame**:\n",
    "   - The updated `merged_data` DataFrame is displayed using `print(merged_data.head())`, which shows the first few rows of the DataFrame with the updated `flight_id` and timestamp columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "934017b2-de88-439b-86f6-18e925a2136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  alert  \\\n",
      "0           108       79969       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "1           109       79970       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "2           110       79971       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "3           111       79972       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "4           112       79973       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "\n",
      "   altitude  count  geoaltitude  ...  squawk           timestamp      track  \\\n",
      "0    1275.0      2       1550.0  ...     NaN 2017-08-01 07:18:29  315.00000   \n",
      "1    1325.0      2       1600.0  ...     NaN 2017-08-01 07:18:30  293.19858   \n",
      "2    1325.0      2       1625.0  ...     NaN 2017-08-01 07:18:31  294.44397   \n",
      "3    1300.0      2       1625.0  ...     NaN 2017-08-01 07:18:32  294.44397   \n",
      "4    1300.0      2       1600.0  ...     NaN 2017-08-01 07:18:33  291.25052   \n",
      "\n",
      "   vertical_rate           flight_id   cumdist  compute_gs  compute_track  \\\n",
      "0          576.0  3E0F76_969_2017_08  0.000000   10.327583      270.00003   \n",
      "1          320.0  3E0F76_969_2017_08  0.002869   10.327583      270.00003   \n",
      "2           64.0  3E0F76_969_2017_08  0.002869    0.000000      180.00000   \n",
      "3           64.0  3E0F76_969_2017_08  0.011918   32.575943      287.99356   \n",
      "4         -192.0  3E0F76_969_2017_08  0.011918    0.000000      180.00000   \n",
      "\n",
      "  month  time_diff  \n",
      "0     8        NaN  \n",
      "1     8        1.0  \n",
      "2     8        1.0  \n",
      "3     8        1.0  \n",
      "4     8        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert timestamp to datetime if not already done\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "\n",
    "# Extract year and month in the format YYYY_MM\n",
    "year_month = merged_data['timestamp'].dt.strftime('%Y_%m')\n",
    "\n",
    "# Modify the flight_id column\n",
    "merged_data['flight_id'] = merged_data['flight_id'] + '_' + year_month\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "168774d4-452c-4068-a00c-aa1a2ef7296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.sort_values(by=['flight_id', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a816ac-301a-4a92-9b4d-11d458d8bd8f",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\"> Column Selection in the DataFrame</span>\n",
    "\n",
    "This code snippet selects a subset of columns from the `merged_data` DataFrame to focus on specific features.\n",
    "\n",
    "#### 1. **Define Columns to Keep**:\n",
    "   - A list, `columns_to_keep`, is created to specify which columns from the original `merged_data` DataFrame should be retained:\n",
    "     - `flight_id`: The unique identifier for each flight.\n",
    "     - `altitude`: The altitude of the aircraft.\n",
    "     - `geoaltitude`: The geometric altitude of the aircraft.\n",
    "     - `groundspeed`: The speed of the aircraft relative to the ground.\n",
    "     - `latitude`: The geographical latitude of the aircraft.\n",
    "     - `longitude`: The geographical longitude of the aircraft.\n",
    "     - `vertical_rate`: The rate of change in altitude (climb or descent rate).\n",
    "     - `compute_gs`: A computed value for ground speed.\n",
    "\n",
    "#### 2. **Subset the DataFrame**:\n",
    "   - Using the list `columns_to_keep`, the DataFrame `df` is created by selecting only the specified columns from `merged_data`.\n",
    "\n",
    "#### 3. **Display the Result**:\n",
    "   - The `.head()` function is used to display the first few rows of the resulting DataFrame (`df`), allowing the user to quickly verify that the correct columns have been retained.\n",
    "\n",
    "This process allows for focusing on a specific set of features, which can be particularly useful when preparing the data for analysis or machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a32b3fcb-7b95-4ad7-8a06-4314d953e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                   flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
       "101055   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101056   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101057   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101058   3DD7B9_007_2017_09     -25.0        -50.0         44.0  53.504489   \n",
       "101059   3DD7B9_007_2017_09     -25.0        -50.0         44.0  53.504489   \n",
       "...                     ...       ...          ...          ...        ...   \n",
       "1832681  RDF29_2110_2021_07    5050.0       5550.0         89.0  51.491547   \n",
       "1832683  RDF29_2110_2021_07    5025.0       5500.0         90.0  51.491318   \n",
       "1832685  RDF29_2110_2021_07    5000.0       5450.0         91.0  51.491197   \n",
       "1832687  RDF29_2110_2021_07    4925.0       5375.0         91.0  51.490825   \n",
       "1832689  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
       "\n",
       "         longitude  vertical_rate  compute_gs  \n",
       "101055   10.170916         -256.0         NaN  \n",
       "101056   10.170916         -256.0    0.000000  \n",
       "101057   10.170916         -256.0    0.000000  \n",
       "101058   10.170916         -256.0    0.000000  \n",
       "101059   10.170916         -256.0    0.000000  \n",
       "...            ...            ...         ...  \n",
       "1832681   9.160371        -2240.0   78.208565  \n",
       "1832683   9.159777        -2368.0   94.224470  \n",
       "1832685   9.159546        -2432.0   40.602660  \n",
       "1832687   9.158630        -2432.0  147.530010  \n",
       "1832689   9.158366        -2496.0   47.926025  \n",
       "\n",
       "[2177006 rows x 8 columns]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the columns to keep as the input changes\n",
    "columns_to_keep = ['flight_id', 'altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude', 'vertical_rate', 'compute_track']\n",
    "df = merged_data[columns_to_keep]\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae6bfd-4b33-408e-b068-e0a2e12bdc5b",
   "metadata": {},
   "source": [
    "#  <u style=\"text-decoration-thickness: 4px;\">Missing Value Analysis</span>\n",
    "\n",
    "This code snippet identifies and summarizes the missing values in the `df` DataFrame, and calculates the percentage of missing data for each column.\n",
    "\n",
    "#### 1. **Identifying Missing Values**:\n",
    "   - The `df.isnull().sum()` function is used to identify missing values in the DataFrame. It returns the count of missing values (`NaN`) for each column.\n",
    "\n",
    "#### 2. **Displaying the Missing Values Count**:\n",
    "   - The count of missing values is printed to provide insight into which columns have missing data and how many missing entries they contain.\n",
    "\n",
    "#### 3. **Calculating the Percentage of Missing Values**:\n",
    "   - The percentage of missing values for each column is calculated by dividing the count of missing values (`missing_summary`) by the total number of rows (`len(df)`), and then multiplying by 100.\n",
    "   - This helps to understand the proportion of missing data in each column relative to the total dataset.\n",
    "\n",
    "The result allows the user to assess the quality of the data and determine whether any preprocessing or imputation is needed before further analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bd53f3a-1b4e-4100-ac4c-4fc57c59ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id            0\n",
      "altitude         13916\n",
      "geoaltitude      26632\n",
      "groundspeed      12173\n",
      "latitude             0\n",
      "longitude            0\n",
      "vertical_rate    12306\n",
      "compute_gs        1383\n",
      "dtype: int64\n",
      "flight_id        0.000000\n",
      "altitude         0.639227\n",
      "geoaltitude      1.223331\n",
      "groundspeed      0.559162\n",
      "latitude         0.000000\n",
      "longitude        0.000000\n",
      "vertical_rate    0.565272\n",
      "compute_gs       0.063528\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# finding the missing value and percentage of data\n",
    "# Summarize missing data\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary)\n",
    "# Percentage of missing values\n",
    "print((missing_summary / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce0a4c-8509-4449-997b-21f09174a9b1",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Missing Value Imputation with Moving Average</span>\n",
    "\n",
    "This code snippet demonstrates how to handle missing values in a DataFrame by applying a moving average for imputation on numeric columns and using forward and backward filling to handle remaining NaNs.\n",
    "\n",
    "#### 1. **Define Constants**:\n",
    "   - The `window_size` variable is defined as `10`, specifying the number of periods over which the moving average is calculated.\n",
    "\n",
    "#### 2. **Track Missing Values Before and After Imputation**:\n",
    "   - The `total_missing_before` dictionary stores the initial count of missing values for each column.\n",
    "   - The `total_missing_after` dictionary is initialized with zero values for all columns and will be updated after imputation.\n",
    "\n",
    "#### 3. **Handling Numeric Columns**:\n",
    "   - The `numeric_cols` variable stores the list of columns that contain numeric data (using `df.select_dtypes(include=[\"number\"])`).\n",
    "   - For each numeric column, the following imputation steps are performed:\n",
    "     - **Moving Average**: Missing values are filled with the rolling mean of the column, using a window size of `10`. The `min_periods=1` ensures that the average is calculated even if there are fewer than 10 available data points.\n",
    "     - **Forward Fill**: Remaining missing values are forward filled, using the previous non-null value.\n",
    "     - **Backward Fill**: Any remaining NaNs are backward filled, using the next available non-null value.\n",
    "\n",
    "#### 4. **Handling Non-Numeric Columns**:\n",
    "   - For non-numeric columns (if any), the missing values count is updated, but no imputation is applied.\n",
    "\n",
    "#### 5. **Display Results**:\n",
    "   - After imputation, the total number of missing values for each column is printed, showing how many NaNs remain in each column after the imputation process.\n",
    "\n",
    "#### 6. **Optional Saving**:\n",
    "   - The code includes a commented-out line (`df.to_csv(\"imputed_data.csv\", index=False)`) to save the imputed DataFrame to a CSV file if needed.\n",
    "\n",
    "This process ensures that missing values in numeric columns are filled based on a smoothed representation of the data, allowing for a more complete dataset suitable for analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd984573-58fc-4d5e-8d9a-876593d72b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67/1179182108.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(\n",
      "/tmp/ipykernel_67/1179182108.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col] = df[col].fillna(method=\"ffill\")\n",
      "/tmp/ipykernel_67/1179182108.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(method=\"ffill\")\n",
      "/tmp/ipykernel_67/1179182108.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col] = df[col].fillna(method=\"bfill\")\n",
      "/tmp/ipykernel_67/1179182108.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(method=\"bfill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NaN counts per column:\n",
      "Feature 'flight_id':\n",
      "  NaN after imputation: 0\n",
      "Feature 'altitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'geoaltitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'groundspeed':\n",
      "  NaN after imputation: 0\n",
      "Feature 'latitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'longitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'vertical_rate':\n",
      "  NaN after imputation: 0\n",
      "Feature 'compute_gs':\n",
      "  NaN after imputation: 0\n",
      "\n",
      "Imputation completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define constants\n",
    "window_size = 10  # Window size for moving average\n",
    "\n",
    "# Initialize total NaN counts dictionaries\n",
    "total_missing_before = df.isnull().sum()\n",
    "total_missing_after = {col: 0 for col in df.columns}  # Initialize for all columns\n",
    "\n",
    "# Select only numeric columns for rolling mean\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Process each column\n",
    "for col in numeric_cols:\n",
    "    # Apply moving average to impute missing values\n",
    "    df[col] = df[col].fillna(\n",
    "        df[col].rolling(window=window_size, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Forward fill for remaining NaNs\n",
    "    df[col] = df[col].fillna(method=\"ffill\")\n",
    "    \n",
    "    # Backward fill for remaining NaNs\n",
    "    df[col] = df[col].fillna(method=\"bfill\")\n",
    "    \n",
    "    # Update total NaN counts after imputation\n",
    "    total_missing_after[col] = df[col].isnull().sum()\n",
    "\n",
    "# Handle non-numeric columns: update total_missing_after for consistency\n",
    "for col in df.columns:\n",
    "    if col not in numeric_cols:\n",
    "        total_missing_after[col] = df[col].isnull().sum()\n",
    "\n",
    "# Print total NaN counts before and after imputation\n",
    "print(\"\\nTotal NaN counts per column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"Feature '{col}':\")\n",
    "    print(f\"  NaN after imputation: {total_missing_after[col]}\")\n",
    "\n",
    "# Save the imputed DataFrame if needed\n",
    "# df.to_csv(\"imputed_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nImputation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b145c9-99ed-4cee-a5ae-078ca22ba3ae",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Missing Data</span>\n",
    "\n",
    "This code calculates and displays the total number of missing (null) values in each column of the dataframe `df`. \n",
    "\n",
    "1. **`df.isnull()`**: This function checks for missing values in the dataframe `df` and returns a boolean dataframe where `True` indicates a missing value and `False` indicates a non-missing value.\n",
    "2. **`.sum()`**: This function is applied to the boolean dataframe to count the number of `True` values (missing values) in each column.\n",
    "3. **`missing_summary`**: The result is stored in the variable `missing_summary`, which contains the total count of missing values for each column.\n",
    "4. **`print(missing_summary)`**: This line outputs the `missing_summary` to the console, allowing users to easily view the count of missing values in each column of the dataframe.\n",
    "\n",
    "This helps to quickly identify columns with missing data, which can be crucial for data cleaning and preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35b5bc9-a30f-4915-8ae6-acc89488f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id        0\n",
      "altitude         0\n",
      "geoaltitude      0\n",
      "groundspeed      0\n",
      "latitude         0\n",
      "longitude        0\n",
      "vertical_rate    0\n",
      "compute_gs       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summarize missing data\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90620358-1237-400f-b668-e763bc14df5f",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Counting Unique Values in the 'flight_id' Column</span>\n",
    "\n",
    "This code calculates the number of unique flight IDs in the `'flight_id'` column of the dataframe `df`.\n",
    "\n",
    "1. **`df['flight_id']`**: This accesses the `'flight_id'` column in the dataframe `df`.\n",
    "2. **`.nunique()`**: This function returns the number of unique values in the specified column. It counts the distinct entries in the `'flight_id'` column.\n",
    "3. **`num_unique_flight_ids`**: The result is stored in the variable `num_unique_flight_ids`, which contains the count of unique flight IDs.\n",
    "4. **`print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")`**: This line outputs the number of unique flight IDs to the console, allowing the user to quickly see how many different flight IDs are present in the dataset.\n",
    "\n",
    "This code is useful for understanding the diversity or repetition of flight IDs in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "870871b2-d822-4c4c-9118-1ce874f34853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique flight IDs: 5324\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique values in the 'flight_id' column\n",
    "num_unique_flight_ids = df['flight_id'].nunique()\n",
    "\n",
    "print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08957a15-9c3f-4179-a6a1-30c135098747",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Counting and Extracting Unique Flight IDs</span>\n",
    "\n",
    "This code counts the number of unique flight IDs and extracts the distinct flight IDs from the `'flight_id'` column in the dataframe `df`.\n",
    "\n",
    "1. **Counting Unique Flight IDs**:\n",
    "   - **`df['flight_id'].nunique()`**: This function calculates the number of unique flight IDs in the `'flight_id'` column of the dataframe `df`.\n",
    "   - **`num_unique_flight_ids`**: The count of unique flight IDs is stored in the variable `num_unique_flight_ids`.\n",
    "   - **`print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")`**: The total number of unique flight IDs is printed to the console.\n",
    "\n",
    "2. **Extracting Unique Flight IDs**:\n",
    "   - **`df['flight_id'].unique()`**: This function extracts the unique flight IDs from the `'flight_id'` column as an array and stores them in the variable `all_unique_ids`.\n",
    "   - A commented-out section (`#print(\"\\nList of unique flight IDs:\")`) shows an option to print the list of unique flight IDs for inspection.\n",
    "\n",
    "3. **Viewing the Updated DataFrame**:\n",
    "   - **`print(\"Updated DataFrame:\")`**: A message indicating that the updated version of the dataframe will be printed.\n",
    "   - **`print(df.tail())`**: This line prints the last 5 rows of the dataframe (`df`) to the console, which helps to inspect the recent changes or values.\n",
    "\n",
    "This code helps in understanding the distribution of flight IDs in the dataset and allows for easy extraction and review of distinct flight IDs. It also includes an option to view the most recent entries in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "478ff0c6-af87-42e4-8f34-b0e5d5151fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique flight IDs: 5324\n",
      "Updated DataFrame:\n",
      "                  flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "1832681  RDF29_2110_2021_07    5050.0       5550.0         89.0  51.491547   \n",
      "1832683  RDF29_2110_2021_07    5025.0       5500.0         90.0  51.491318   \n",
      "1832685  RDF29_2110_2021_07    5000.0       5450.0         91.0  51.491197   \n",
      "1832687  RDF29_2110_2021_07    4925.0       5375.0         91.0  51.490825   \n",
      "1832689  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
      "\n",
      "         longitude  vertical_rate  compute_gs  \n",
      "1832681   9.160371        -2240.0   78.208565  \n",
      "1832683   9.159777        -2368.0   94.224470  \n",
      "1832685   9.159546        -2432.0   40.602660  \n",
      "1832687   9.158630        -2432.0  147.530010  \n",
      "1832689   9.158366        -2496.0   47.926025  \n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique values in the 'flight_id' column\n",
    "num_unique_flight_ids = df['flight_id'].nunique()\n",
    "print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")\n",
    "\n",
    "# Extract unique flight IDs\n",
    "all_unique_ids = df['flight_id'].unique()\n",
    "\n",
    "# If you want to save the mapping for later use:\n",
    "#print(\"\\nList of unique flight IDs:\")\n",
    "#print(all_unique_ids)\n",
    "\n",
    "\n",
    "print(\"Updated DataFrame:\")\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6c7fb-5cef-4664-bfae-08c42f39babb",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Extracting the Last Record for Each Flight ID</span>\n",
    "\n",
    "This code groups the dataframe by the `'flight_id'` column and extracts the last record for each unique flight ID.\n",
    "\n",
    "1. **Grouping by Flight ID**:\n",
    "   - **`df.groupby('flight_id').last()`**: This groups the dataframe by the `'flight_id'` column and retrieves the last entry for each group (i.e., the last record for each unique flight ID).\n",
    "   - **`.reset_index()`**: This resets the index of the resulting dataframe after the groupby operation, ensuring that the flight IDs are treated as regular columns rather than indices.\n",
    "\n",
    "2. **Reordering Columns**:\n",
    "   - **`df_last = df_last[df.columns]`**: This line ensures that the columns in the new dataframe `df_last` match the original column order in the original dataframe `df`.\n",
    "\n",
    "3. **Displaying Data**:\n",
    "   - **`print(df_last.head())`**: This prints the first 5 rows of the `df_last` dataframe, allowing you to inspect the results of the operation.\n",
    "   - **`print(df_last.tail())`**: This prints the last 5 rows of the `df_last` dataframe, providing a quick way to verify the final entries.\n",
    "\n",
    "This code is useful for extracting the most recent or final record for each flight ID in a dataset, which can be important for analyzing the latest available data for each flight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61ac7911-7aa2-4dbf-b132-d80a9aaca329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "0  3DD7B9_007_2017_09     425.0        400.0        138.0  53.353683   \n",
      "1  3DD7B9_042_2017_08     550.0        525.0         99.0  53.411291   \n",
      "2  3DD7B9_073_2017_08    1200.0       1200.0         93.0  53.408295   \n",
      "3  3DD7B9_075_2017_08     950.0        950.0        126.0  53.315094   \n",
      "4  3DD7B9_080_2017_08     525.0        475.0         96.0  53.393188   \n",
      "\n",
      "   longitude  vertical_rate  compute_gs  \n",
      "0   9.804984            0.0         0.0  \n",
      "1   9.892381            0.0         0.0  \n",
      "2   9.891384          448.0         0.0  \n",
      "3   9.771005         -128.0         0.0  \n",
      "4   9.839669            0.0         0.0  \n",
      "               flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "5319  DHXCB_1762_2021_07     775.0       1150.0        125.0  50.750931   \n",
      "5320  RDF17_1583_2021_07    2775.0       2975.0        115.0  51.370488   \n",
      "5321  RDF17_2108_2021_07    2575.0       2900.0         87.0  51.685043   \n",
      "5322  RDF29_2109_2021_07    1775.0       2125.0        115.0  51.385477   \n",
      "5323  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
      "\n",
      "      longitude  vertical_rate  compute_gs  \n",
      "5319   7.109045         -384.0  128.199650  \n",
      "5320   9.188614         -256.0    0.000000  \n",
      "5321   9.203351         -192.0    0.000000  \n",
      "5322   9.327774         -384.0  192.254440  \n",
      "5323   9.158366        -2496.0   47.926025  \n"
     ]
    }
   ],
   "source": [
    "#taking the last values\n",
    "df_last = df.groupby('flight_id').last().reset_index()\n",
    "\n",
    "df_last = df_last[df.columns]\n",
    "\n",
    "print(df_last.head())\n",
    "print(df_last.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa46c5-f557-4a54-84e0-ab1d577a65a8",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Saving Flight IDs and Dropping the 'flight_id' Column</span>\n",
    "\n",
    "This code extracts the `'flight_id'` values and removes the `'flight_id'` column from the dataframe `df_last` to create a new dataframe with scaled data (or without the `flight_id`).\n",
    "\n",
    "1. **Saving Flight IDs**:\n",
    "   - **`saved_flight_ids = df_last[\"flight_id\"].values`**: This line saves all the flight ID values from the `'flight_id'` column of `df_last` into the `saved_flight_ids` variable. The `.values` attribute returns a NumPy array of flight IDs, maintaining their order.\n",
    "\n",
    "2. **Dropping the 'flight_id' Column**:\n",
    "   - **`df_last_scaled_df = df_last.drop(columns=['flight_id'])`**: This removes the `'flight_id'` column from the dataframe `df_last`, creating a new dataframe `df_last_scaled_df`. The resulting dataframe no longer includes the flight ID values, which is typically done before applying transformations like scaling or normalization.\n",
    "\n",
    "3. **Displaying the New DataFrame**:\n",
    "   - **`print(df_last_scaled_df.head())`**: This prints the first 5 rows of the new dataframe `df_last_scaled_df`, allowing inspection of the dataset without the `'flight_id'` column.\n",
    "\n",
    "This code is typically used when preparing data for modeling or further analysis, where flight IDs are preserved separately, but the main dataframe is simplified by removing non-numeric or identifier columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c832a89-6f81-476c-b292-6501b741338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitude  geoaltitude  groundspeed   latitude  longitude  vertical_rate  \\\n",
      "0     425.0        400.0        138.0  53.353683   9.804984            0.0   \n",
      "1     550.0        525.0         99.0  53.411291   9.892381            0.0   \n",
      "2    1200.0       1200.0         93.0  53.408295   9.891384          448.0   \n",
      "3     950.0        950.0        126.0  53.315094   9.771005         -128.0   \n",
      "4     525.0        475.0         96.0  53.393188   9.839669            0.0   \n",
      "\n",
      "   compute_gs  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#flight_ids = df_last['flight_id'].values\n",
    "saved_flight_ids = df_last[\"flight_id\"].values  # Save all flight_id values in order\n",
    "df_last_scaled_df = df_last.drop(columns=['flight_id'])\n",
    "print(df_last_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340a60e-7414-4ecd-b9aa-00868064c7d0",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Converting Geographic Coordinates to 3D Cartesian Coordinates</span>\n",
    "\n",
    "This code transforms geographic coordinates (latitude and longitude) from degrees to 3D Cartesian coordinates (x, y, z) using spherical coordinate conversion.\n",
    "\n",
    "1. **Converting Latitude and Longitude to Radians**:\n",
    "   - **`df_last_scaled_df['lat_rad'] = np.radians(df_last_scaled_df['latitude'])`**: This line converts the `'latitude'` column from degrees to radians using NumPy's `radians` function.\n",
    "   - **`df_last_scaled_df['lon_rad'] = np.radians(df_last_scaled_df['longitude'])`**: Similarly, this line converts the `'longitude'` column from degrees to radians.\n",
    "\n",
    "2. **Computing the Cartesian Coordinates**:\n",
    "   - **`df_last_scaled_df['x'] = np.cos(df_last_scaled_df['lat_rad']) * np.cos(df_last_scaled_df['lon_rad'])`**: This calculates the `x` coordinate using the formula for spherical to Cartesian transformation.\n",
    "   - **`df_last_scaled_df['y'] = np.cos(df_last_scaled_df['lat_rad']) * np.sin(df_last_scaled_df['lon_rad'])`**: This calculates the `y` coordinate using the sine of the longitude and cosine of the latitude.\n",
    "   - **`df_last_scaled_df['z'] = np.sin(df_last_scaled_df['lat_rad'])`**: This calculates the `z` coordinate, which is the sine of the latitude in radians.\n",
    "\n",
    "3. **Dropping Intermediate Columns**:\n",
    "   - **`df_last_scaled_df.drop(columns=['lat_rad', 'lon_rad','latitude', 'longitude'], inplace=True)`**: After the conversion, the intermediate columns for radians and original latitude/longitude are dropped, as they are no longer necessary.\n",
    "\n",
    "4. **Displaying the Transformed DataFrame**:\n",
    "   - **`print(df_last_scaled_df.head())`**: This prints the first 5 rows of the transformed dataframe, which now contains the `x`, `y`, and `z` coordinates instead of the original latitude and longitude.\n",
    "\n",
    "This code is commonly used for geospatial data transformations, especially when working with geographic coordinates for 3D visualizations, modeling, or distance computations on a spherical Earth model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c786351-3ea3-485f-8373-5969066f6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitude  geoaltitude  groundspeed  vertical_rate  compute_gs         x  \\\n",
      "0     425.0        400.0        138.0            0.0         0.0  0.588155   \n",
      "1     550.0        525.0         99.0            0.0         0.0  0.587204   \n",
      "2    1200.0       1200.0         93.0          448.0         0.0  0.587248   \n",
      "3     950.0        950.0        126.0         -128.0         0.0  0.588748   \n",
      "4     525.0        475.0         96.0            0.0         0.0  0.587548   \n",
      "\n",
      "          y         z  \n",
      "0  0.101645  0.802335  \n",
      "1  0.102403  0.802935  \n",
      "2  0.102400  0.802904  \n",
      "3  0.101388  0.801933  \n",
      "4  0.101906  0.802747  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'latitude' and 'longitude' columns in degrees\n",
    "df_last_scaled_df['lat_rad'] = np.radians(df_last_scaled_df['latitude'])  # Convert latitude to radians\n",
    "df_last_scaled_df['lon_rad'] = np.radians(df_last_scaled_df['longitude'])  # Convert longitude to radians\n",
    "\n",
    "# Compute x, y, z coordinates\n",
    "df_last_scaled_df['x'] = np.cos(df_last_scaled_df['lat_rad']) * np.cos(df_last_scaled_df['lon_rad'])\n",
    "df_last_scaled_df['y'] = np.cos(df_last_scaled_df['lat_rad']) * np.sin(df_last_scaled_df['lon_rad'])\n",
    "df_last_scaled_df['z'] = np.sin(df_last_scaled_df['lat_rad'])\n",
    "\n",
    "# Drop intermediate radian columns if not needed\n",
    "df_last_scaled_df.drop(columns=['lat_rad', 'lon_rad','latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(df_last_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd69563-32cb-43a7-8933-3618e09222fb",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Preparing Feature and Target Variables</span>\n",
    "\n",
    "This code extracts the feature matrix `X` and the target variable(s) `y` from the transformed dataframe `df_last_scaled_df`, and prints their shapes to verify the structure.\n",
    "\n",
    "1. **Extracting Features (X)**:\n",
    "   - **`X = df_last_scaled_df.values`**: This line extracts all the values from the dataframe `df_last_scaled_df` into a NumPy array and assigns it to the variable `X`. This will include all the columns, treating them as features for machine learning or other analytical tasks.\n",
    "\n",
    "2. **Extracting Target Variables (y)**:\n",
    "   - **`y = df_last_scaled_df.iloc[:, [0, 5, 6, 7]].values`**: This line selects specific columns (the 1st, 6th, 7th, and 8th) from `df_last_scaled_df` using `.iloc[]` and extracts their values as the target variables. The selected columns are stored in the `y` variable. These columns are likely being treated as the output or labels in a supervised learning task.\n",
    "\n",
    "3. **Printing the Shapes**:\n",
    "   - **`print(\"X shape:\", X.shape)`**: This prints the shape of the feature matrix `X`, showing the number of rows (samples) and columns (features).\n",
    "   - **`print(\"y shape:\", y.shape)`**: This prints the shape of the target variable(s) `y`, showing the number of rows (samples) and the number of columns (target variables).\n",
    "\n",
    "This code is used to prepare the data for machine learning or statistical analysis by separating the features (input variables) and target variables (output labels), while also verifying their structure through the printed shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0fd28be-efca-4aca-b9c9-1ab2cf4c40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5324, 8)\n",
      "y shape: (5324, 4)\n"
     ]
    }
   ],
   "source": [
    "X = df_last_scaled_df.values  \n",
    "y = df_last_scaled_df.iloc[:, [0, 5, 6,7]].values \n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65765247-81e7-4626-afb1-4545743566bc",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Loading Pre-trained Scalers for Data Transformation</span>\n",
    "\n",
    "This code loads pre-trained scaling models from disk to transform the features and target variables during the machine learning process.\n",
    "\n",
    "1. **Loading Scalers for Features and Targets**:\n",
    "   - **`scaler_X_altitude = joblib.load('scaler_X_altitude.pkl')`**: This line loads the pre-trained scaler for the `X` features related to altitude from the file `'scaler_X_altitude.pkl'`. The scaler is expected to be used for transforming the feature data related to altitude, such as standardizing or normalizing values.\n",
    "   - **`scaler_X_other = joblib.load('scaler_X_other.pkl')`**: Similarly, this loads the pre-trained scaler for other features (not related to altitude) from the file `'scaler_X_other.pkl'`.\n",
    "   - **`scaler_y_altitude = joblib.load('scaler_y_altitude.pkl')`**: This line loads the pre-trained scaler for the target variable related to altitude from `'scaler_y_altitude.pkl'`, which will be used for transforming the target values corresponding to altitude.\n",
    "   - **`scaler_y_other = joblib.load('scaler_y_other.pkl')`**: This loads the pre-trained scaler for other target variables from `'scaler_y_other.pkl'`.\n",
    "\n",
    "2. **Purpose of Scalers**:\n",
    "   - The loaded scalers are used to standardize or normalize the data before applying machine learning models. These scalers ensure that the features and targets are transformed consistently according to the parameters (such as mean and standard deviation) used during the training of the model.\n",
    "\n",
    "This approach allows for reusability of scaling operations in subsequent steps, ensuring that the model can be applied to new data in the same manner as it was originally trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ab641b-c9f1-457a-a291-679fe02c44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "scaler_X_altitude = joblib.load('scaler_X_altitude.pkl')\n",
    "scaler_X_other = joblib.load('scaler_X_other.pkl')\n",
    "scaler_y_altitude = joblib.load('scaler_y_altitude.pkl')\n",
    "scaler_y_other = joblib.load('scaler_y_other.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955abe0c-4d50-4e8f-84de-a6549ef620f3",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Separating, Scaling, and Preparing Data for LSTM Input</span>\n",
    "\n",
    "This code preprocesses the feature and target variables by separating, scaling, and reshaping the data for input into a Long Short-Term Memory (LSTM) model.\n",
    "\n",
    "1. **Identifying the Altitude Column**:\n",
    "   - **`altitude_index = 0`**: The index of the altitude column is defined. This index is used to separate altitude-related data from the other features and targets. Modify the index based on the dataset's structure if needed.\n",
    "\n",
    "2. **Separating the Altitude Data**:\n",
    "   - **`altitude_X = X[:, altitude_index].reshape(-1, 1)`**: The altitude feature (from `X`) is extracted and reshaped to a column vector for later scaling.\n",
    "   - **`X_other = X[:, [i for i in range(X.shape[1]) if i != altitude_index]]`**: All other features (excluding altitude) are stored in `X_other`.\n",
    "\n",
    "3. **Separating the Altitude Target**:\n",
    "   - **`altitude_y = y[:, altitude_index].reshape(-1, 1)`**: The target variable related to altitude is extracted and reshaped to a column vector.\n",
    "   - **`y_other = y[:, [i for i in range(y.shape[1]) if i != altitude_index]]`**: The remaining target variables are stored in `y_other`.\n",
    "\n",
    "4. **Scaling the Features and Targets**:\n",
    "   - **`altitude_X_scaled = scaler_X_altitude.transform(altitude_X)`**: The altitude feature in `X` is scaled to the range [0, 1] using the pre-trained scaler `scaler_X_altitude`.\n",
    "   - **`X_other_scaled = scaler_X_other.transform(X_other)`**: Other features in `X` are scaled to the range [-1, 1] using the pre-trained scaler `scaler_X_other`.\n",
    "   - **`altitude_y_scaled = scaler_y_altitude.transform(altitude_y)`**: The altitude target in `y` is scaled to the range [0, 1] using the pre-trained scaler `scaler_y_altitude`.\n",
    "   - **`y_other_scaled = scaler_y_other.transform(y_other)`**: The other targets in `y` are scaled to the range [-1, 1] using the pre-trained scaler `scaler_y_other`.\n",
    "\n",
    "5. **Recombining Scaled Data**:\n",
    "   - **`X = np.hstack([altitude_X_scaled, X_other_scaled])`**: The scaled altitude feature is recombined with the other scaled features into a new feature matrix `X`.\n",
    "   - **`y = np.hstack([altitude_y_scaled, y_other_scaled])`**: The scaled altitude target is recombined with the other scaled targets into a new target array `y`.\n",
    "\n",
    "6. **Expanding Dimensions for LSTM Input**:\n",
    "   - **`X = np.expand_dims(X, axis=1)`**: LSTM models expect input data to be of shape `(samples, time_steps, features)`. This reshapes `X` to include a single time step, making the shape `(samples, 1, features)`.\n",
    "\n",
    "7. **Ensuring y is a NumPy Array**:\n",
    "   - **`y = y.values if isinstance(y, pd.Series) else y`**: Ensures that `y` is a NumPy array, as LSTM models require NumPy arrays for input.\n",
    "\n",
    "8. **Printing the Shapes**:\n",
    "   - **`print(\"X shape after expansion:\", X.shape)`**: This prints the shape of `X` after the reshaping operation.\n",
    "   - **`print(\"y shape:\", y.shape)`**: This prints the shape of `y`, confirming the correct dimensionality for model input.\n",
    "\n",
    "This code ensures that the features and targets are appropriately separated, scaled, and reshaped for input into an LSTM model, which is commonly used for sequence prediction tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b26775cb-38a1-4fc9-ac97-cd5fed2e4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after expansion: (5324, 1, 8)\n",
      "y shape: (5324, 4)\n"
     ]
    }
   ],
   "source": [
    "# Identify the altitude column index\n",
    "altitude_index = 0  # Change this based on your dataset\n",
    "\n",
    "# Separate the altitude column from X (features)\n",
    "altitude_X = X[:, altitude_index].reshape(-1, 1)\n",
    "X_other = X[:, [i for i in range(X.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Separate the altitude column from y (target)\n",
    "altitude_y = y[:, altitude_index].reshape(-1, 1)\n",
    "y_other = y[:, [i for i in range(y.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# **Use the SAME scalers from training**\n",
    "# Scale the altitude feature in X to [0, 1]\n",
    "altitude_X_scaled = scaler_X_altitude.transform(altitude_X)\n",
    "\n",
    "# Scale the other features in X to [-1, 1]\n",
    "X_other_scaled = scaler_X_other.transform(X_other)\n",
    "\n",
    "# Scale the altitude in y to [0, 1]\n",
    "altitude_y_scaled = scaler_y_altitude.transform(altitude_y)\n",
    "\n",
    "# Scale the other columns in y to [-1, 1]\n",
    "y_other_scaled = scaler_y_other.transform(y_other)\n",
    "\n",
    "# **Recombine scaled altitude with other features**\n",
    "X = np.hstack([altitude_X_scaled, X_other_scaled])\n",
    "y = np.hstack([altitude_y_scaled, y_other_scaled])\n",
    "\n",
    "# **Expand dimensions for LSTM input (samples, time_steps, features)**\n",
    "X = np.expand_dims(X, axis=1)  # Shape becomes (samples, 1, features)  \n",
    "\n",
    "# Ensure y is a NumPy array\n",
    "y = y.values if isinstance(y, pd.Series) else y  \n",
    "\n",
    "# Print the shapes\n",
    "print(\"X shape after expansion:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506cbc7-1d8d-4b6b-8967-36c1be6e208b",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Making Predictions for the Next 60 Seconds and Saving Results</span>\n",
    "\n",
    "This code makes predictions for the next 60 seconds based on the input data and stores the predicted values, including altitude, latitude, longitude, and other variables, along with the corresponding `flight_id`. The predictions are then saved into a CSV file for further analysis.\n",
    "\n",
    "1. **Initializing a List for Predictions**:\n",
    "   - **`predictions = []`**: This list will store the predictions for each row of data, including the predicted values for altitude, latitude, longitude, and any other relevant variables.\n",
    "\n",
    "2. **Iterating Over Each Row in `X`**:\n",
    "   - **`for idx in range(len(X)):`**: This loop iterates over every row in the `X` matrix, which represents the features for each sample.\n",
    "   \n",
    "3. **Preparing the Input for Prediction**:\n",
    "   - **`last_input = np.zeros((1, 1, 8))`**: Initializes an array `last_input` with zeros, shaped to match the model's expected input format, i.e., `(samples, time_steps, features)`. The shape here is `(1, 1, 8)` to represent 1 sample with 1 time step and 8 features.\n",
    "   - **`last_input[0, 0, :] = X[idx, :]`**: Populates the `last_input` array with the feature values for the current row in `X`.\n",
    "\n",
    "4. **Making Predictions**:\n",
    "   - **`next_prediction = model.predict(last_input, verbose=0)`**: This predicts the next 60 seconds of data (such as altitude, latitude, etc.) for the current row using the trained model.\n",
    "\n",
    "5. **Storing Predictions**:\n",
    "   - The predicted values (altitude, latitude, longitude, and other features) are stored in the `predictions` list with the corresponding `flight_id`. \n",
    "   - The dictionary stores each predicted feature under meaningful names like `\"predicted_altitude\"`, `\"predicted_x\"`, and `\"predicted_y\"`. \n",
    "\n",
    "6. **Converting Predictions to a DataFrame**:\n",
    "   - **`predictions_df = pd.DataFrame(predictions)`**: Converts the list of dictionaries into a DataFrame for easy manipulation and analysis.\n",
    "\n",
    "7. **Restoring the `flight_id`**:\n",
    "   - **`predictions_df[\"flight_id\"] = saved_flight_ids`**: Restores the original `flight_id` column, ensuring the predictions are associated with the correct flight IDs.\n",
    "   \n",
    "8. **Reordering Columns**:\n",
    "   - **`predictions_df = predictions_df[[\"flight_id\", \"predicted_altitude\", \"predicted_x\", \"predicted_y\", \"predicted_z\"]]`**: Reorders the columns to display the `flight_id` first, followed by the predicted values for altitude, latitude, longitude, and other features.\n",
    "\n",
    "9. **Saving Predictions to a CSV File**:\n",
    "   - **`predictions_df.to_csv('row_wise_predictions.csv', index=False)`**: Saves the DataFrame containing predictions to a CSV file for further use. (Note: this line is commented out in the code but can be enabled to save the predictions.)\n",
    "   \n",
    "10. **Final Output**:\n",
    "    - **`print(\"Predictions saved to 'row_wise_predictions.csv'\")`**: This print statement confirms that the predictions have been saved.\n",
    "\n",
    "This code is designed to run predictions for each row of input data using a trained model, store the results in a structured format, and save the predictions to a CSV file for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64b6e011-aeab-4592-801b-3280f15e338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'row_wise_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over each row in X\n",
    "for idx in range(len(X)):  # Iterate through all rows in the dataset\n",
    "    # Initialize last_input with the correct shape for 7 features\n",
    "    last_input = np.zeros((1, 1, 8))  # Match the input shape of the model\n",
    "    last_input[0, 0, :] = X[idx, :]  # Populate all 7 features from the current row\n",
    "\n",
    "    # Predict 60 seconds into the future for this row\n",
    "    next_prediction = model.predict(last_input, verbose=0)  # Predict the 60-second value\n",
    "\n",
    "\n",
    "    # Save predictions with flight_id and predicted values\n",
    "    predictions.append({\n",
    "        \"predicted_altitude\": next_prediction[0][0],  # Predicted altitude\n",
    "        \"predicted_x\": next_prediction[0][1],  # Predicted latitude\n",
    "        \"predicted_y\": next_prediction[0][2],  # Predicted longitude\n",
    "        \"predicted_z\":next_prediction[0][3]\n",
    "        # Add other predicted features if necessary\n",
    "    })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Restore the flight_id column\n",
    "predictions_df[\"flight_id\"] = saved_flight_ids  # Ensuring original order\n",
    "\n",
    "# Reorder columns for better readability\n",
    "predictions_df = predictions_df[[\"flight_id\", \"predicted_altitude\", \"predicted_x\", \"predicted_y\",\"predicted_z\"]]\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "#predictions_df.to_csv('row_wise_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'row_wise_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c113254-1121-4ca8-a51f-e567f34779da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>predicted_altitude</th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>predicted_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>DHXCB_1762_2021_07</td>\n",
       "      <td>0.021725</td>\n",
       "      <td>0.674062</td>\n",
       "      <td>0.212727</td>\n",
       "      <td>0.296856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>RDF17_1583_2021_07</td>\n",
       "      <td>0.020973</td>\n",
       "      <td>0.655071</td>\n",
       "      <td>0.233320</td>\n",
       "      <td>0.339777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>RDF17_2108_2021_07</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.640503</td>\n",
       "      <td>0.235079</td>\n",
       "      <td>0.384064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>RDF29_2109_2021_07</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.652798</td>\n",
       "      <td>0.238901</td>\n",
       "      <td>0.342042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>RDF29_2110_2021_07</td>\n",
       "      <td>0.020852</td>\n",
       "      <td>0.652501</td>\n",
       "      <td>0.234566</td>\n",
       "      <td>0.346771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               flight_id  predicted_altitude  predicted_x  predicted_y  \\\n",
       "5319  DHXCB_1762_2021_07            0.021725     0.674062     0.212727   \n",
       "5320  RDF17_1583_2021_07            0.020973     0.655071     0.233320   \n",
       "5321  RDF17_2108_2021_07            0.020091     0.640503     0.235079   \n",
       "5322  RDF29_2109_2021_07            0.020793     0.652798     0.238901   \n",
       "5323  RDF29_2110_2021_07            0.020852     0.652501     0.234566   \n",
       "\n",
       "      predicted_z  \n",
       "5319     0.296856  \n",
       "5320     0.339777  \n",
       "5321     0.384064  \n",
       "5322     0.342042  \n",
       "5323     0.346771  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()\n",
    "predictions_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce090aa-8c1e-4203-9730-9367380407ff",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Inverse Scaling of Predictions and Data Preprocessing</span>\n",
    "\n",
    "This code performs the inverse transformation of the scaled predictions to restore the predicted values for altitude, latitude, longitude, and other features back to their original ranges.\n",
    "\n",
    "1. **Identifying the Altitude Column Index**:\n",
    "   - **`altitude_index = 1`**: Specifies the index of the altitude column. The index can be adjusted depending on where the altitude data is located in the dataset. In this case, it assumes altitude is in the second position (index 1).\n",
    "\n",
    "2. **Extracting the Predicted Values**:\n",
    "   - **`predicted_altitude = predictions_df[['predicted_altitude']].values`**: This line extracts the predicted altitude values from the `predictions_df` DataFrame and stores them as a 2D array for later rescaling.\n",
    "   - **`predicted_other = predictions_df[['predicted_x', 'predicted_y', 'predicted_z']].values`**: Extracts the predicted values for latitude, longitude, and any other features (such as altitude in 3D space or other spatial coordinates).\n",
    "\n",
    "3. **Performing the Inverse Transformation**:\n",
    "   - **`rescaled_altitude = scaler_y_altitude.inverse_transform(predicted_altitude)`**: This line applies the inverse transformation using the pre-trained scaler `scaler_y_altitude` to rescale the predicted altitude values back to their original range.\n",
    "   - **`rescaled_other = scaler_y_other.inverse_transform(predicted_other)`**: Similarly, the predicted latitude, longitude, and other features are rescaled using the pre-trained scaler `scaler_y_other` to restore them to their original values.\n",
    "\n",
    "4. **Replacing the Original Prediction Columns with Rescaled Values**:\n",
    "   - **`predictions_df['rescaled_altitude'] = rescaled_altitude`**: The rescaled altitude values are added to the DataFrame as a new column `rescaled_altitude`.\n",
    "   - **`predictions_df[['rescaled_x', 'rescaled_y', 'rescaled_z']] = rescaled_other`**: The rescaled values for latitude, longitude, and other features are added to the DataFrame under new column names (`rescaled_x`, `rescaled_y`, `rescaled_z`).\n",
    "\n",
    "5. **Optionally Dropping the Old Scaled Prediction Columns**:\n",
    "   - **`predictions_df.drop(columns=['predicted_altitude', 'predicted_x', 'predicted_y', 'predicted_z'], inplace=True)`**: The original scaled prediction columns are dropped from the DataFrame as they are no longer needed after rescaling.\n",
    "\n",
    "6. **Verifying the Results**:\n",
    "   - **`print(predictions_df.head())`**: Prints the first 5 rows of the updated DataFrame to verify that the rescaled values are correctly added.\n",
    "   - **`print(predictions_df.tail())`**: Prints the last 5 rows to further verify the result.\n",
    "\n",
    "This code is crucial for converting the scaled predictions back to their original values, making them more interpretable and suitable for further analysis or reporting. After inverse transformation, the predicted values reflect real-world measurements (such as altitude and geographical coordinates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "072946b6-522f-43c9-812c-94a90a1fcb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  rescaled_altitude  rescaled_x  rescaled_y  rescaled_z\n",
      "0  3DD7B9_007_2017_09         699.877563    0.586225    0.103837    0.803171\n",
      "1  3DD7B9_042_2017_08         703.252380    0.585793    0.104077    0.803480\n",
      "2  3DD7B9_073_2017_08         703.340332    0.585527    0.104184    0.803667\n",
      "3  3DD7B9_075_2017_08         702.617004    0.586547    0.103751    0.802948\n",
      "4  3DD7B9_080_2017_08         701.963501    0.586011    0.103962    0.803326\n",
      "               flight_id  rescaled_altitude  rescaled_x  rescaled_y  \\\n",
      "5319  DHXCB_1762_2021_07        1580.817749    0.627347    0.081743   \n",
      "5320  RDF17_1583_2021_07        1484.539551    0.619005    0.096268   \n",
      "5321  RDF17_2108_2021_07        1371.697144    0.612606    0.097508   \n",
      "5322  RDF29_2109_2021_07        1461.520142    0.618006    0.100204   \n",
      "5323  RDF29_2110_2021_07        1469.118164    0.617876    0.097147   \n",
      "\n",
      "      rescaled_z  \n",
      "5319    0.774204  \n",
      "5320    0.778970  \n",
      "5321    0.783887  \n",
      "5322    0.779221  \n",
      "5323    0.779746  \n"
     ]
    }
   ],
   "source": [
    "# Identify column indices\n",
    "altitude_index = 1  # Change this if altitude is in a different position\n",
    "\n",
    "# Extract predicted values\n",
    "predicted_altitude = predictions_df[['predicted_altitude']].values  # Extract altitude as 2D array\n",
    "predicted_other = predictions_df[['predicted_x', 'predicted_y','predicted_z']].values  # Extract other features\n",
    "\n",
    "# **Perform inverse transformation**\n",
    "rescaled_altitude = scaler_y_altitude.inverse_transform(predicted_altitude)  # Rescale altitude (0,1 → original range)\n",
    "rescaled_other = scaler_y_other.inverse_transform(predicted_other)  # Rescale other features (-1,1 → original range)\n",
    "\n",
    "# **Replace the original columns with the rescaled values**\n",
    "predictions_df['rescaled_altitude'] = rescaled_altitude  # Add rescaled altitude\n",
    "predictions_df[['rescaled_x', 'rescaled_y','rescaled_z']] = rescaled_other  # Add rescaled lat/lon\n",
    "\n",
    "# **Optionally drop the old scaled prediction columns**\n",
    "predictions_df.drop(columns=['predicted_altitude', 'predicted_x', 'predicted_y','predicted_z'], inplace=True)\n",
    "\n",
    "# **Verify the results**\n",
    "print(predictions_df.head())\n",
    "print(predictions_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c6539-84de-4f28-9bb9-907660e521ea",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Restoring Latitude and Longitude from 3D Coordinates</span>\n",
    "\n",
    "This code calculates the latitude and longitude from the previously rescaled 3D Cartesian coordinates (x, y, z) and updates the `predictions_df` DataFrame accordingly. The `x`, `y`, and `z` columns are then dropped if they are no longer needed.\n",
    "\n",
    "1. **Computing Latitude from z**:\n",
    "   - **`predictions_df['latitude'] = np.degrees(np.arcsin(predictions_df['rescaled_z']))`**: This line calculates the latitude using the arcsine (`np.arcsin`) of the rescaled `z` values. The result is in radians, which is then converted to degrees using `np.degrees`. The latitude is derived from the `z` coordinate in the 3D Cartesian system, assuming the data represents a spherical or ellipsoidal model.\n",
    "\n",
    "2. **Computing Longitude from x and y**:\n",
    "   - **`predictions_df['longitude'] = np.degrees(np.arctan2(predictions_df['rescaled_y'], predictions_df['rescaled_x']))`**: This line calculates the longitude using the `arctan2` function, which computes the arctangent of the ratio of `y` to `x` to obtain the correct angle (in radians) representing the longitude. The result is then converted to degrees using `np.degrees`. This step assumes the data is represented in a 2D projection or 3D Cartesian coordinates, where `x` and `y` map to geographical coordinates.\n",
    "\n",
    "3. **Dropping the Original `x`, `y`, and `z` Columns**:\n",
    "   - **`predictions_df.drop(columns=['rescaled_x', 'rescaled_y', 'rescaled_z'], inplace=True)`**: The rescaled `x`, `y`, and `z` columns are dropped from the DataFrame since they are no longer needed after calculating the latitude and longitude.\n",
    "\n",
    "4. **Displaying the Updated DataFrame**:\n",
    "   - **`print(predictions_df.head())`**: This line prints the first 5 rows of the updated DataFrame to verify that the latitude and longitude have been correctly restored.\n",
    "\n",
    "This code is useful for converting the 3D spatial coordinates back into latitude and longitude values, which are commonly used for mapping or geographic analyses. The resulting `latitude` and `longitude` values are now in degrees, ready for further use or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e30b38db-56d3-4cb9-a8ce-daae0733a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  rescaled_altitude   latitude  longitude\n",
      "0  3DD7B9_007_2017_09         699.877563  53.433998  10.044492\n",
      "1  3DD7B9_042_2017_08         703.252380  53.463711  10.074492\n",
      "2  3DD7B9_073_2017_08         703.340332  53.481689  10.089153\n",
      "3  3DD7B9_075_2017_08         702.617004  53.412563  10.030974\n",
      "4  3DD7B9_080_2017_08         701.963501  53.448883  10.059967\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute latitude from z\n",
    "predictions_df['latitude'] = np.degrees(np.arcsin(predictions_df['rescaled_z']))\n",
    "\n",
    "# Compute longitude from x and y\n",
    "predictions_df['longitude'] = np.degrees(np.arctan2(predictions_df['rescaled_y'], predictions_df['rescaled_x']))\n",
    "\n",
    "# Drop x, y, z if not needed\n",
    "predictions_df.drop(columns=['rescaled_x', 'rescaled_y', 'rescaled_z'], inplace=True)\n",
    "\n",
    "# Display the DataFrame with restored lat/lon\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cbcbb-d520-4b6e-a2e3-5b397483c96f",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Merging Predictions with Original Data Based on Flight ID and Timestamp</span>\n",
    "\n",
    "This code merges the predictions DataFrame (`predictions_df`) with the original data (`original_order_df`) based on a combination of `flight_id` and `timestamp`. It includes creating a new identifier and performing the merge operation.\n",
    "\n",
    "1. **Loading the Original Flight Data**:\n",
    "   - **`original_order_df = pd.read_csv(\"/home/jovyan/data/predictions_format.csv\")`**: This line loads the original dataset containing flight information, including flight IDs and timestamps, from a CSV file.\n",
    "\n",
    "2. **Converting Timestamp to Datetime Format**:\n",
    "   - **`original_order_df['timestamp'] = pd.to_datetime(original_order_df['timestamp'])`**: The `timestamp` column is converted to the `datetime` format to facilitate time-based operations like extracting the year and month.\n",
    "\n",
    "3. **Extracting Year and Month**:\n",
    "   - **`original_order_df['year_month'] = original_order_df['timestamp'].dt.strftime('%Y_%m')`**: This extracts the year and month in the `YYYY_MM` format from the `timestamp` column and stores it in a new column `year_month`. This allows grouping or merging based on time intervals.\n",
    "\n",
    "4. **Creating a New `flight_id_rename` Column**:\n",
    "   - **`original_order_df['flight_id_rename'] = original_order_df['flight_id'].astype(str) + '_' + original_order_df['year_month']`**: This creates a new column `flight_id_rename` by concatenating the `flight_id` with the `year_month` value, forming a unique identifier that combines both flight ID and the year-month pair.\n",
    "\n",
    "5. **Merging Predictions with the Original Data**:\n",
    "   - **`merged_df = original_order_df.merge(predictions_df, left_on='flight_id_rename', right_on='flight_id', how='left')`**: This merges `original_order_df` with `predictions_df` using the `flight_id_rename` column in `original_order_df` and the `flight_id` column in `predictions_df`. The merge is done using a \"left\" join, ensuring that all rows from `original_order_df` are kept, and matching rows from `predictions_df` are added.\n",
    "\n",
    "6. **Dropping the `flight_id_rename` Column**:\n",
    "   - **`merged_df.drop(columns=['flight_id_rename'], inplace=True)`**: After the merge, the temporary `flight_id_rename` column is no longer needed, so it is dropped from the DataFrame.\n",
    "\n",
    "7. **Displaying the Updated DataFrame**:\n",
    "   - **`print(merged_df.head())`**: This prints the first 5 rows of the merged DataFrame, showing the original flight data with the corresponding predictions.\n",
    "\n",
    "This process allows you to combine the predictions with the original flight data by aligning the flight IDs and the year-month values, ensuring that the predictions match the correct flight records based on both flight ID and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58312dee-f7ab-4a50-ae41-7a7f71f3d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  flight_id_x           timestamp  latitude_x  longitude_x  altitude  \\\n",
      "0  3E0F76_969 2017-08-01 07:25:23           0            0         0   \n",
      "1  3E0F76_970 2017-08-01 08:19:52           0            0         0   \n",
      "2  3E0F76_972 2017-08-01 11:26:36           0            0         0   \n",
      "3  3E0F76_973 2017-08-01 12:00:52           0            0         0   \n",
      "4  3E0F76_974 2017-08-01 13:42:23           0            0         0   \n",
      "\n",
      "  year_month         flight_id_y  rescaled_altitude  latitude_y  longitude_y  \n",
      "0    2017_08  3E0F76_969_2017_08        1943.071655   48.893475    11.815664  \n",
      "1    2017_08  3E0F76_970_2017_08        1909.218506   48.960136    11.863684  \n",
      "2    2017_08  3E0F76_972_2017_08        1958.378540   48.872608    11.338322  \n",
      "3    2017_08  3E0F76_973_2017_08        1955.404297   48.879944    11.226740  \n",
      "4    2017_08  3E0F76_974_2017_08        2053.909180   48.694546    11.682269  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original flight ID order and timestamp\n",
    "original_order_df = pd.read_csv(\"/home/jovyan/data/predictions_format.csv\")\n",
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "original_order_df['timestamp'] = pd.to_datetime(original_order_df['timestamp'])\n",
    "\n",
    "# Extract year and month in the format YYYY_MM\n",
    "original_order_df['year_month'] = original_order_df['timestamp'].dt.strftime('%Y_%m')\n",
    "\n",
    "# Create a new column 'flight_id_rename'\n",
    "original_order_df['flight_id_rename'] = original_order_df['flight_id'].astype(str) + '_' + original_order_df['year_month']\n",
    "\n",
    "# Merge predictions_df into original_order_df using 'flight_id_rename'\n",
    "merged_df = original_order_df.merge(\n",
    "    predictions_df, \n",
    "    left_on='flight_id_rename', \n",
    "    right_on='flight_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the 'flight_id_rename' column as it's no longer needed\n",
    "merged_df.drop(columns=['flight_id_rename'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771256f-7c95-45c0-b489-5e86d5dc2d16",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Saving Merged Predictions to a CSV File</span>\n",
    "\n",
    "This code saves the merged predictions DataFrame (`merged_df`) into a CSV file for future analysis or reporting.\n",
    "\n",
    "1. **Saving the Merged DataFrame to CSV**:\n",
    "   - **`merged_df.to_csv('rescaled_predictions.csv', index=False)`**: This line saves the `merged_df` DataFrame, which contains both the original flight data and the rescaled predictions, to a CSV file named `rescaled_predictions.csv`. The `index=False` argument ensures that the row indices are not included in the saved CSV file.\n",
    "\n",
    "2. **Printing Confirmation**:\n",
    "   - **`print(\"Rescaled predictions saved to 'rescaled_predictions.csv'\")`**: After successfully saving the file, this print statement confirms that the predictions have been saved to the specified file.\n",
    "\n",
    "By saving the merged DataFrame, you can preserve the predictions along with the corresponding flight data, making it easier to share, analyze, or further process the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "004a049e-5dcd-437a-91f5-019d21d13b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled predictions saved to 'rescaled_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "merged_df.to_csv('rescaled_predictions.csv', index=False)\n",
    "\n",
    "print(\"Rescaled predictions saved to 'rescaled_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665a3ed-766d-403f-bdfa-b5a6b44a451f",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Loading and Displaying the Rescaled Predictions</span>\n",
    "\n",
    "This code loads the previously saved rescaled predictions from a CSV file and displays the first few rows of the data.\n",
    "\n",
    "1. **Loading the CSV File**:\n",
    "   - **`df = pd.read_csv('rescaled_predictions.csv')`**: This line loads the `rescaled_predictions.csv` file into a Pandas DataFrame named `df`. This file contains the merged flight data and rescaled predictions.\n",
    "\n",
    "2. **Displaying the First Few Rows**:\n",
    "   - **`df.head()`**: This function displays the first 5 rows of the DataFrame `df` to give a quick overview of the loaded data. It helps verify the content and structure of the rescaled predictions file.\n",
    "\n",
    "By loading and displaying the data, you can ensure that the saved predictions were properly stored and can now be further analyzed or processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0094968-ee72-4356-a9b3-b01e6992c2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id_x</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>latitude_x</th>\n",
       "      <th>longitude_x</th>\n",
       "      <th>altitude</th>\n",
       "      <th>year_month</th>\n",
       "      <th>flight_id_y</th>\n",
       "      <th>rescaled_altitude</th>\n",
       "      <th>latitude_y</th>\n",
       "      <th>longitude_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3E0F76_969</td>\n",
       "      <td>2017-08-01 07:25:23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017_08</td>\n",
       "      <td>3E0F76_969_2017_08</td>\n",
       "      <td>1943.0717</td>\n",
       "      <td>48.893475</td>\n",
       "      <td>11.815664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3E0F76_970</td>\n",
       "      <td>2017-08-01 08:19:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017_08</td>\n",
       "      <td>3E0F76_970_2017_08</td>\n",
       "      <td>1909.2185</td>\n",
       "      <td>48.960136</td>\n",
       "      <td>11.863684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3E0F76_972</td>\n",
       "      <td>2017-08-01 11:26:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017_08</td>\n",
       "      <td>3E0F76_972_2017_08</td>\n",
       "      <td>1958.3785</td>\n",
       "      <td>48.872610</td>\n",
       "      <td>11.338322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3E0F76_973</td>\n",
       "      <td>2017-08-01 12:00:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017_08</td>\n",
       "      <td>3E0F76_973_2017_08</td>\n",
       "      <td>1955.4043</td>\n",
       "      <td>48.879944</td>\n",
       "      <td>11.226740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3E0F76_974</td>\n",
       "      <td>2017-08-01 13:42:23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017_08</td>\n",
       "      <td>3E0F76_974_2017_08</td>\n",
       "      <td>2053.9092</td>\n",
       "      <td>48.694546</td>\n",
       "      <td>11.682269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flight_id_x            timestamp  latitude_x  longitude_x  altitude  \\\n",
       "0  3E0F76_969  2017-08-01 07:25:23           0            0         0   \n",
       "1  3E0F76_970  2017-08-01 08:19:52           0            0         0   \n",
       "2  3E0F76_972  2017-08-01 11:26:36           0            0         0   \n",
       "3  3E0F76_973  2017-08-01 12:00:52           0            0         0   \n",
       "4  3E0F76_974  2017-08-01 13:42:23           0            0         0   \n",
       "\n",
       "  year_month         flight_id_y  rescaled_altitude  latitude_y  longitude_y  \n",
       "0    2017_08  3E0F76_969_2017_08          1943.0717   48.893475    11.815664  \n",
       "1    2017_08  3E0F76_970_2017_08          1909.2185   48.960136    11.863684  \n",
       "2    2017_08  3E0F76_972_2017_08          1958.3785   48.872610    11.338322  \n",
       "3    2017_08  3E0F76_973_2017_08          1955.4043   48.879944    11.226740  \n",
       "4    2017_08  3E0F76_974_2017_08          2053.9092   48.694546    11.682269  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('rescaled_predictions.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8a83f-85e8-464d-b3e3-e86496214885",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\"> Cleaning the DataFrame by Dropping Unwanted Columns</span>\n",
    "\n",
    "This code removes specific columns from the DataFrame (`df`) that are no longer needed for further analysis.\n",
    "\n",
    "1. **Defining the Columns to Drop**:\n",
    "   - **`columns_to_delete = ['latitude_x', 'longitude_x', 'altitude', 'year_month', 'flight_id_y']`**: A list of columns that should be removed from the DataFrame is defined. These columns are identified as unnecessary for the analysis or have duplicate information.\n",
    "\n",
    "2. **Dropping the Specified Columns**:\n",
    "   - **`df_cleaned = df.drop(columns=columns_to_delete)`**: This line drops the columns specified in the `columns_to_delete` list from the DataFrame `df`. The resulting DataFrame, with the unwanted columns removed, is stored in a new variable `df_cleaned`.\n",
    "\n",
    "The cleaned DataFrame (`df_cleaned`) now contains only the relevant columns, making it ready for further processing or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "366f310e-71fd-43f7-9fb8-be76ae0f11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_delete = ['latitude_x', 'longitude_x','altitude','year_month','flight_id_y']\n",
    "\n",
    "df_cleaned = df.drop(columns=columns_to_delete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c7dae-ecbf-4e84-a6b1-45480f4815db",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Renaming Columns in the DataFrame</span>\n",
    "\n",
    "This code renames specific columns in the cleaned DataFrame (`df_cleaned`) to match the desired column names.\n",
    "\n",
    "1. **Creating a Dictionary for Column Renaming**:\n",
    "   - **`columns_to_rename = {'flight_id_x': 'flight_id', 'rescaled_altitude': 'altitude', 'latitude_y': 'latitude', 'longitude_y': 'longitude'}`**: A dictionary is created, where the keys represent the old column names, and the values represent the new column names. This allows for a clear mapping from the old names to the new ones.\n",
    "\n",
    "2. **Renaming the Columns**:\n",
    "   - **`df_cleaned = df_cleaned.rename(columns=columns_to_rename)`**: The `rename` function is used to rename the columns in `df_cleaned` based on the dictionary `columns_to_rename`. The DataFrame is updated in place, and the renamed DataFrame is stored back into `df_cleaned`.\n",
    "\n",
    "By renaming the columns, the DataFrame is now standardized and has more meaningful names, making it easier to interpret and work with the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8304a013-223a-45ed-99f5-d7fac5a74a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping old column names to new ones\n",
    "columns_to_rename = {\n",
    "    'flight_id_x': 'flight_id',\n",
    "    'rescaled_altitude': 'altitude',\n",
    "    'latitude_y': 'latitude',\n",
    "    'longitude_y': 'longitude'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df_cleaned = df_cleaned.rename(columns=columns_to_rename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef42c74-977d-485f-9f69-1cfd8839c087",
   "metadata": {},
   "source": [
    "# <u style=\"text-decoration-thickness: 4px;\">Saving the Cleaned DataFrame to a CSV File</span>\n",
    "\n",
    "This code saves the cleaned and renamed DataFrame (`df_cleaned`) to a new CSV file for further use or sharing.\n",
    "\n",
    "1. **Saving the Cleaned DataFrame**:\n",
    "   - **`df_cleaned.to_csv('final_predictions_group32.csv', index=False)`**: This line saves the `df_cleaned` DataFrame to a CSV file named `final_predictions_group32.csv`. The `index=False` argument ensures that the row indices are not included in the saved file.\n",
    "\n",
    "2. **Confirmation Message**:\n",
    "   - **`print(\"Cleaned CSV saved as 'final_predictions_group32.csv'\")`**: After saving the file, this print statement confirms that the cleaned DataFrame has been successfully saved with the specified filename.\n",
    "\n",
    "By saving the cleaned DataFrame, you create a final version of the predictions that is ready for reporting, further analysis, or sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20ba720c-baeb-46a2-8ba6-aeacf805dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved as 'cleaned_file.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv('final_predictions_group32.csv', index=False)\n",
    "\n",
    "print(\"Cleaned CSV saved as 'final_predictions_group32.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92d5bf-f1b4-44ff-b141-3e24454fd08a",
   "metadata": {},
   "source": [
    "# 5. Presentation and Evaluation of Results\n",
    "\n",
    "Results can be produced given the technical implementation of the presented concept and the database. These results need to be evaluated and verified. Subsequently, it has to be evaluated whether and with which quality the results can fulfill the desired objectives.\n",
    "\n",
    "The following questions and topics might be used as a guidance for the presentation of the results:\n",
    "\n",
    "Use meaningful figures to present the results and also cover intermediate results\n",
    "What findings can be derived with regard to the objectives?\n",
    "The following questions and topics might be used as a guidance for the evaluation of the results:\n",
    "\n",
    "Evaluate the algorithms with common metrics\n",
    "How robust is the implemented methodology?\n",
    "How can the performance of the methodology be classified with regard to the achieved results?\n",
    "Can reliable statements be made on the basis of the methodology?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ca486-e8ec-4f45-bc0a-f0a135d7544e",
   "metadata": {},
   "source": [
    "## 6. Applicability Analysis and Outlook<a class=\"anchor\" id=\"outlook\"></a>\n",
    "-----------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
